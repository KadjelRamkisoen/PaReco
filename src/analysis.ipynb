{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./..')\n",
    "\n",
    "import csv\n",
    "import pickle\n",
    "import Methods.totals as totals\n",
    "import Methods.analysis as analysis\n",
    "import Methods.common as common\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"sorted_dataset_bugfix.pkl\", \"rb\") as f:\n",
    "    dataset_dict = pickle.load(f)\n",
    "    \n",
    "import functools\n",
    "import operator\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Getting totals\n",
    "    Getting the final classification for every PR and calculating totals\n",
    "\"\"\"\n",
    "repos = 342\n",
    "for repo_file in range(0, repos):\n",
    "    variant1 = dataset_dict[repo_file]['source']\n",
    "    variant2 = dataset_dict[repo_file]['destination']\n",
    "    \n",
    "    print(variant1, '-', variant2)\n",
    "    \n",
    "    try:\n",
    "        all_results = common.readResults(repo_file, variant1)\n",
    "        \n",
    "        pr_class = totals.final_class(all_results[0])\n",
    "        all_counts = totals.count_all_classifications(pr_class)\n",
    "\n",
    "        with open('Repos_totals/'+ str(repo_file) + '_' + variant1.split('/')[0] + '_' + variant1.split('/')[1] + '_totals.pkl', 'wb') as f:\n",
    "            pickle.dump([pr_class, all_counts],f)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Plotting the distribution of classifications\n",
    "\"\"\"\n",
    "\n",
    "total_all = 0\n",
    "total_mo_all = 0\n",
    "total_ed_all = 0\n",
    "total_sp_all = 0\n",
    "total_na_all = 0\n",
    "\n",
    "all_results = []\n",
    "\n",
    "repos = 342\n",
    "for repo_file in range(0,repos):\n",
    "    try:\n",
    "        variant1 = dataset_dict[repo_file]['source']\n",
    "        variant2 = dataset_dict[repo_file]['destination']\n",
    "        variant1_prs = dataset_dict[repo_file]['pr'].split('/')\n",
    "\n",
    "        all_totals= totals.read_totals(repo_file, variant1)\n",
    "\n",
    "        total_NA = 0\n",
    "        total_ED = 0\n",
    "        total_MO = 0\n",
    "        total_CC = 0\n",
    "        total_SP= 0\n",
    "        total_NE = 0\n",
    "        total_ERROR = 0\n",
    "\n",
    "        for pr in all_totals[0]:\n",
    "            verdict = all_totals[0][pr]['class']\n",
    "            if verdict == 'ED':\n",
    "                total_ED += 1\n",
    "            elif verdict =='MO':\n",
    "                total_MO += 1\n",
    "            elif verdict == 'SP':\n",
    "                total_SP += 1\n",
    "            elif verdict == 'NA':\n",
    "                total_NA += 1\n",
    "            elif verdict == 'CC':\n",
    "                total_CC += 1\n",
    "            elif verdict =='NE':\n",
    "                total_NE += 1\n",
    "            elif verdict == 'ERROR':\n",
    "                total_ERROR += 1\n",
    "                \n",
    "            total_mid = total_MO+ total_ED + total_SP\n",
    "            total_all += total_mid\n",
    "            \n",
    "        total_total =len(variant1_prs)\n",
    "        \n",
    "        total_mo_all += total_MO\n",
    "        total_ed_all += total_ED\n",
    "        total_sp_all += total_SP\n",
    "        total_na_all += total_NA\n",
    "\n",
    "        totals_list = [total_MO, total_ED, total_SP, total_CC, total_NE, total_NA, total_ERROR]\n",
    "\n",
    "#         analysis.all_class_bar(totals_list, repo_file, variant1, variant2, False)\n",
    "#         analysis.all_class_pie(totals_list, repo_file, variant1, variant2, False)\n",
    "\n",
    "        all_results.append([variant1, variant2, repo_file, total_MO, total_ED, total_SP, total_CC, total_NE, total_NA, total_ERROR])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(variant1)\n",
    "        print(\"Exception: \", e, '\\n\\n')\n",
    "\n",
    "header = [\"variant1\", \"variant2\", \"repo_file\", \"total_MO\", \"total_ED\", \"total_SP\", \"total_CC\", \"total_NE\", \"total_NA\",\" total_ERROR\"]\n",
    "with open(\"results.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    # write the header\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # write multiple rows\n",
    "    writer.writerows(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MO = ',total_mo_all)\n",
    "print('ED = ',total_ed_all)\n",
    "print('SP = ',total_sp_all)\n",
    "print('NI = ', total_na_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "47 + 54 + 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dict(dict1, dict2):\n",
    "    return {**dict1, **dict2}\n",
    "\n",
    "directory = 'Repos_results_bugfix'\n",
    "\n",
    "totals_ = {}\n",
    "repo_nr = 0\n",
    "\n",
    "pair_set = set()\n",
    "\n",
    "i = 0\n",
    "prev_pair_id = '0'\n",
    "\n",
    "all_pr_class_list = []\n",
    "all_counts_list = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    pair_id = file.split('_')[0]\n",
    "    pair_set.add(pair_id)\n",
    "    if pair_id != prev_pair_id:\n",
    "        dict_final ={}\n",
    "        for i in all_pr_class_list:\n",
    "            dict_final=merge_dict(dict_final, i)\n",
    "        \n",
    "        try:\n",
    "            all_counts_all = dict(functools.reduce(operator.add,map(collections.Counter, all_counts_list)))\n",
    "            totals_[pair_id] ={}\n",
    "            totals_[pair_id]['pr_class'] = dict_final\n",
    "            totals_[pair_id]['all_counts'] = all_counts_all\n",
    "        except:\n",
    "            print(pair_id)\n",
    "            print(all_counts_list)\n",
    "            totals_[pair_id] ={}\n",
    "            totals_[pair_id]['pr_class'] = dict_final\n",
    "            totals_[pair_id]['all_counts'] = []\n",
    "            \n",
    "        all_pr_class_list = []\n",
    "        all_counts_list = []\n",
    "        \n",
    "        prev_pair_id = pair_id\n",
    "    with open(f'{directory}/{file}', 'rb') as f:\n",
    "        try:\n",
    "            all_results = pickle.load(f)[0]\n",
    "\n",
    "            pr_class = totals.final_class(all_results)\n",
    "            all_pr_class_list.append(pr_class)\n",
    "            all_counts = totals.count_all_classifications(pr_class)\n",
    "            all_counts_list.append(all_counts)\n",
    "        except EOFError:\n",
    "            continue\n",
    "#             with open('Repos_totals/'+ str(repo_file) + '_' + variant1.split('/')[0] + '_' + variant1.split('/')[1] + '_totals.pkl', 'wb') as f:\n",
    "#                 pickle.dump([pr_class, all_counts],f)\n",
    "#             except Exception as e:\n",
    "#                 print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Plotting the distribution of classifications\n",
    "\"\"\"\n",
    "\n",
    "total_all = 0\n",
    "total_mo_all = 0\n",
    "total_ed_all = 0\n",
    "total_sp_all = 0\n",
    "total_cc_all = 0\n",
    "total_ne_all = 0\n",
    "total_error_all = 0\n",
    "total_na_all = 0\n",
    "\n",
    "all_results = []\n",
    "\n",
    "repos = 342\n",
    "for repo_file in totals_:\n",
    "    try:\n",
    "        variant1 = dataset_dict[int(repo_file)]['source']\n",
    "        variant2 = dataset_dict[int(repo_file)]['destination']\n",
    "        variant1_prs = dataset_dict[int(repo_file)]['pr'].split('/')\n",
    "\n",
    "        all_totals= totals_[repo_file]['pr_class']\n",
    "\n",
    "        total_NA = 0\n",
    "        total_ED = 0\n",
    "        total_MO = 0\n",
    "        total_CC = 0\n",
    "        total_SP= 0\n",
    "        total_NE = 0\n",
    "        total_ERROR = 0\n",
    "\n",
    "        for pr in all_totals:\n",
    "            verdict = all_totals[pr]['class']\n",
    "            if verdict == 'ED':\n",
    "                total_ED += 1\n",
    "            elif verdict =='MO':\n",
    "                total_MO += 1\n",
    "            elif verdict == 'SP':\n",
    "                total_SP += 1\n",
    "            elif verdict == 'NA':\n",
    "                total_NA += 1\n",
    "            elif verdict == 'CC':\n",
    "                total_CC += 1\n",
    "            elif verdict =='NE':\n",
    "                total_NE += 1\n",
    "            elif verdict == 'ERROR':\n",
    "                total_ERROR += 1\n",
    "                \n",
    "            total_mid = total_MO+ total_ED + total_SP\n",
    "            total_all += total_mid\n",
    "            \n",
    "        total_total =len(variant1_prs)\n",
    "        \n",
    "        total_mo_all += total_MO\n",
    "        total_ed_all += total_ED\n",
    "        total_sp_all += total_SP\n",
    "        total_na_all += total_NA\n",
    "\n",
    "        total_cc_all += total_CC\n",
    "        total_ne_all += total_NE\n",
    "        total_error_all += total_ERROR\n",
    "\n",
    "        totals_list = [total_MO, total_ED, total_SP, total_CC, total_NE, total_NA, total_ERROR]\n",
    "\n",
    "#         analysis.all_class_bar(totals_list, repo_file, variant1, variant2, False)\n",
    "#         analysis.all_class_pie(totals_list, repo_file, variant1, variant2, False)\n",
    "\n",
    "        all_results.append([variant1, variant2, repo_file, total_MO, total_ED, total_SP, total_CC, total_NE, total_NA, total_ERROR])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(variant1)\n",
    "        print(\"Exception: \", e, '\\n\\n')\n",
    "\n",
    "# header = [\"variant1\", \"variant2\", \"repo_file\", \"total_MO\", \"total_ED\", \"total_SP\", \"total_CC\", \"total_NE\", \"total_NA\",\" total_ERROR\"]\n",
    "# with open(\"results_16_05_22.csv\", \"w\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     # write the header\n",
    "#     writer.writerow(header)\n",
    "\n",
    "#     # write multiple rows\n",
    "#     writer.writerows(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MO = ',total_mo_all)\n",
    "print('ED = ',total_ed_all)\n",
    "print('SP = ',total_sp_all)\n",
    "print('NI = ', total_na_all)\n",
    "\n",
    "print('CC = ',total_cc_all)\n",
    "print('NE = ',total_ne_all)\n",
    "print('ERROR = ', total_error_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "total_hist = [total_mo_all, total_ed_all, total_sp_all, total_na_all, total_cc_all, total_ne_all, total_error_all]\n",
    "\n",
    "left = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "plt.figure(figsize=(15,15), dpi=80)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.bar(left, total_hist, tick_label = ['MO', 'ED', 'SP', 'NI', 'CC', 'NE', 'ERROR'], width = 0.8, color = [\"#e41a1c\",\"#377eb8\",\"#4daf4a\",\"#984ea3\", \"#ff7f00\", \"#ffff33\", \"#a65628\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentage interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_classified = total_mo_all+total_ed_all+total_sp_all+total_na_all+total_cc_all+total_error_all+total_ne_all\n",
    "print(f'total classified = {total_classified}')\n",
    "print(f'total interesting = {100*(total_mo_all+total_ed_all+total_sp_all)/(total_classified-45)}%')\n",
    "print(f'total unintersting = {100*total_na_all/total_classified}%')\n",
    "print(f'total error = {100*total_error_all/total_classified}%')\n",
    "print(f'total non existant = {100*total_ne_all/total_classified}%')\n",
    "print(f'total cannot classify = {100*total_cc_all/total_classified}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File classification distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import Methods.classifier as classifier\n",
    "\n",
    "classifications = {}\n",
    "\n",
    "directory = 'Repos_results_bugfix'\n",
    "for file in os.listdir(directory):\n",
    "    try:\n",
    "        with open(f'{directory}/{file}', 'rb') as f:\n",
    "            results_file = pickle.load(f)[0]\n",
    "\n",
    "            pair_id = file.split('_')[0]\n",
    "\n",
    "            if pair_id not in classifications:\n",
    "                classifications[pair_id] = {}\n",
    "\n",
    "            for i in results_file:\n",
    "                classifications[pair_id][i] = {}\n",
    "\n",
    "                try:\n",
    "                    for file_exec in results_file[i]:\n",
    "                        ext = file_exec.split('.')[-1]\n",
    "                        classifications[pair_id][i][file_exec]={}\n",
    "                        classifications[pair_id][i][file_exec]['ext'] = ext\n",
    "                        classifications[pair_id][i][file_exec]['hunks'] = []\n",
    "                        classifications[pair_id][i][file_exec]['class'] = ''\n",
    "                        \n",
    "                        patchClass = results_file[i][file_exec]['result']['patchClass']\n",
    "                        \n",
    "                        classifications[pair_id][i][file_exec]['class'] = patchClass\n",
    "                        \n",
    "                        if patchClass in ['MO', 'ED', 'SP', 'NA']:\n",
    "                            hunk_matches_buggy = results_file[i][file_exec]['result']['hunkMatchesBuggy']\n",
    "                            hunk_matches_patch = results_file[i][file_exec]['result']['hunkMatchesPatch']\n",
    "\n",
    "                            hunk_classifications = []\n",
    "\n",
    "                            if len(hunk_matches_buggy) == 0 or len(hunk_matches_patch)==0:\n",
    "                                if len(hunk_matches_buggy) != 0:\n",
    "                                    for patch_nr in hunk_matches_buggy:\n",
    "                                        class_buggy = hunk_matches_buggy[patch_nr]['class']\n",
    "                                        class_patch = 'MC'\n",
    "\n",
    "                                        hunk_class = classifier.classify_hunk(class_buggy, class_patch)\n",
    "                                        hunk_classifications.append(hunk_class)\n",
    "                                elif len(hunk_matches_patch) != 0:\n",
    "                                    for patch_nr in hunk_matches_buggy:\n",
    "                                        class_buggy = 'MC'\n",
    "                                        class_patch = hunk_matches_patch[patch_nr]['class']\n",
    "\n",
    "                                        hunk_class = classifier.classify_hunk(class_buggy, class_patch)\n",
    "                                        hunk_classifications.append(hunk_class)\n",
    "                                else:\n",
    "                                    class_buggy = 'MC'\n",
    "                                    class_patch = 'MC'\n",
    "                                    hunk_class = classifier.classify_hunk(class_buggy, class_patch)\n",
    "                                    hunk_classifications.append(hunk_class)\n",
    "\n",
    "                            elif len(hunk_matches_buggy) == len(hunk_matches_patch):\n",
    "                                for patch_nr in hunk_matches_buggy:\n",
    "                                    class_buggy = hunk_matches_buggy[patch_nr]['class']\n",
    "                                    class_patch = hunk_matches_patch[patch_nr]['class']\n",
    "\n",
    "                                    hunk_class = classifier.classify_hunk(class_buggy, class_patch)\n",
    "                                    hunk_classifications.append(hunk_class)\n",
    "                            classifications[pair_id][i][file_exec]['hunks'].append(hunk_classifications)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print('Pair: ', pair_id)\n",
    "                    print('file: ', file_exec)\n",
    "                    print('PR: ', i)\n",
    "                    print(e)\n",
    "#                     print(results_file[i][file_exec]['result'])\n",
    "    except EOFError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_file_classes = {'ED':0, 'MO': 0,'SP':0, 'NA':0, 'NE':0, 'CC':0, 'ERROR':0}\n",
    "total_hunk_classes = {'ED':0, 'MO': 0,'SP':0, 'NA':0,}\n",
    "lang_distr = {}\n",
    "\n",
    "for pair in classifications:\n",
    "    for pr in classifications[pair]:\n",
    "        for file in classifications[pair][pr]:\n",
    "            class_ = classifications[pair][pr][file]['class']\n",
    "            if  classifications[pair][pr][file]['ext'] not in lang_distr:\n",
    "                lang_distr[classifications[pair][pr][file]['ext']] = 1\n",
    "            else:\n",
    "                lang_distr[classifications[pair][pr][file]['ext']] += 1\n",
    "                \n",
    "            if class_ == 'OTHER EXT':\n",
    "                class_= 'CC'\n",
    "            if class_ == 'NOT EXISTING':\n",
    "                class_ = 'NE'\n",
    "            try:\n",
    "                total_file_classes[class_] += 1\n",
    "            except:\n",
    "                print(class_)\n",
    "            \n",
    "            for h_list in classifications[pair][pr][file]['hunks']:\n",
    "                for h in h_list:\n",
    "                    total_hunk_classes[h] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_distr_sorted = {k: v for k, v in sorted(lang_distr.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_distr_sorted_list = []\n",
    "for i in lang_distr_sorted:\n",
    "    lang_distr_sorted_list.append([i, lang_distr_sorted[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('languages.csv', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "      \n",
    "    write.writerow(['Extension', 'Count'])\n",
    "    write.writerows(lang_distr_sorted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_hist = [total_file_classes['MO'], total_file_classes['ED'], total_file_classes['SP'], total_file_classes['NA'], total_file_classes['NE'], total_file_classes['CC'], total_file_classes['ERROR']]\n",
    "\n",
    "left = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "plt.figure(figsize=(15,15), dpi=80)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.bar(left, total_hist, tick_label = ['MO', 'ED', 'SP', 'NI', 'NE', 'CC', 'ERROR'], width = 0.8, color = [\"#e41a1c\",\"#377eb8\",\"#4daf4a\",\"#984ea3\", \"#ff7f00\", \"#ffff33\", \"#a65628\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_hist = [total_hunk_classes['MO'], total_hunk_classes['ED'], total_hunk_classes['SP'], total_hunk_classes['NA']]\n",
    "\n",
    "left = [1, 2, 3, 4]\n",
    "\n",
    "plt.figure(figsize=(15,15), dpi=80)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.bar(left, total_hist, tick_label = ['MO', 'ED', 'SP', 'NI'], width = 0.8, color = [\"#e41a1c\",\"#377eb8\",\"#4daf4a\",\"#984ea3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_file_ext_classes = {} #{'ED':0, 'MO': 0,'SP':0, 'NA':0, 'NE':0, 'CC':0, 'ERROR':0}\n",
    "total_hunk_ext_classes = {} #{'ED':0, 'MO': 0,'SP':0, 'NA':0,}\n",
    "\n",
    "py_files = []\n",
    "\n",
    "for pair in classifications:\n",
    "    for pr in classifications[pair]:\n",
    "        for file in classifications[pair][pr]:\n",
    "            class_ = classifications[pair][pr][file]['class']\n",
    "            ext = classifications[pair][pr][file]['ext']\n",
    "            hunks = classifications[pair][pr][file]['hunks']\n",
    "            \n",
    "            if ext not in total_file_ext_classes:\n",
    "                total_file_ext_classes[ext] = {}\n",
    "            \n",
    "            if class_ == 'OTHER EXT':\n",
    "                class_= 'CC'\n",
    "            if class_ == 'NOT EXISTING':\n",
    "                class_ = 'NE'\n",
    "                \n",
    "            if ext == 'py':\n",
    "                py_files.append(class_)\n",
    "            try:\n",
    "                total_file_ext_classes[ext][class_]+=1\n",
    "            except:\n",
    "                total_file_ext_classes[ext][class_] = 1\n",
    "\n",
    "            if ext not in total_hunk_ext_classes:\n",
    "                total_hunk_ext_classes[ext] = {}\n",
    "            for h_list in hunks:\n",
    "                for h in h_list:\n",
    "                    if h not in total_hunk_ext_classes[ext]:\n",
    "                        total_hunk_ext_classes[ext][h] = 1\n",
    "                    total_hunk_ext_classes[ext][h] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_hist_data(file, total_file_ext_classes):\n",
    "    fec = total_file_ext_classes[file]\n",
    "    labels = ['MO', 'ED', 'SP', 'NA', 'NE', 'CC', 'ERROR']\n",
    "    hist = []\n",
    "    for i in labels:\n",
    "        try:\n",
    "            hist.append(fec[i])\n",
    "        except:\n",
    "            hist.append(0)\n",
    "\n",
    "    return hist\n",
    "\n",
    "def plot_hist(file1, file2, file3, total_file_ext_classes):\n",
    "    names = ['MO', 'ED', 'SP', 'NI', 'NE', 'CC', 'ERROR']\n",
    "    \n",
    "    if len(file3)==0 and len(file2)==0:\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(20, 10), sharey=True)\n",
    "        axs.grid()\n",
    "        axs.set_xlabel(file1)\n",
    "        axs.bar(names, get_hist_data(file1, total_file_ext_classes))\n",
    "    else:    \n",
    "        fig, axs = plt.subplots(1, 3, figsize=(20, 10), sharey=True)\n",
    "        axs[0].grid()\n",
    "        axs[0].set_xlabel(file1)\n",
    "        axs[0].bar(names, get_hist_data(file1, total_file_ext_classes))\n",
    "        axs[1].grid()\n",
    "        axs[1].set_xlabel(file2)\n",
    "        axs[1].bar(names, get_hist_data(file2, total_file_ext_classes))\n",
    "        axs[2].grid()\n",
    "        axs[2].set_xlabel(file3)\n",
    "        axs[2].bar(names, get_hist_data(file3, total_file_ext_classes))    \n",
    "               \n",
    "plot_hist('json', 'py', 'c', total_file_ext_classes)\n",
    "plot_hist('rb', 'scala', 'php', total_file_ext_classes)\n",
    "plot_hist('gradle', 'gemfile', 'ipynb', total_file_ext_classes)\n",
    "plot_hist('xml', 'h', 'java', total_file_ext_classes)\n",
    "plot_hist('sh', 'yaml', 'pl', total_file_ext_classes)\n",
    "plot_hist('kt', 'yml', 'cpp', total_file_ext_classes)\n",
    "plot_hist('js', '', '', total_file_ext_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# MO ED SP NA NE\n",
    "list_mo=[]\n",
    "list_ed=[]\n",
    "list_sp=[]\n",
    "list_na=[]\n",
    "list_ne=[]\n",
    "list_lang=[]\n",
    "\n",
    "for lang in total_file_ext_classes:\n",
    "    if lang in ['c', 'h', 'java', 'cpp', 'py', 'pl', 'json', 'rb', 'scala', 'sh', \\\n",
    "                'php', 'gradle', 'gemfile', 'xml', 'kt', 'yml', 'yaml', 'js', 'ipynb', 'txt']:\n",
    "        \n",
    "        list_lang.append(lang)\n",
    "        if 'MO' in total_file_ext_classes[lang]:\n",
    "            list_mo.append(total_file_ext_classes[lang]['MO'])\n",
    "        else:\n",
    "            list_mo.append(0)\n",
    "            \n",
    "        if 'ED' in total_file_ext_classes[lang]:    \n",
    "            list_ed.append(total_file_ext_classes[lang]['ED'])\n",
    "        else:\n",
    "            list_ed.append(0)\n",
    "            \n",
    "        if 'SP' in total_file_ext_classes[lang]:\n",
    "            list_sp.append(total_file_ext_classes[lang]['SP'])\n",
    "        else:\n",
    "            list_sp.append(0)\n",
    "            \n",
    "        if 'NA' in total_file_ext_classes[lang]:\n",
    "            list_na.append(total_file_ext_classes[lang]['NA'])\n",
    "        else:\n",
    "            list_na.append(0)\n",
    "            \n",
    "        if 'NE' in total_file_ext_classes[lang]:\n",
    "            list_ne.append(total_file_ext_classes[lang]['NE'])\n",
    "        else:\n",
    "            list_ne.append(0)\n",
    "            \n",
    "lang_pd = pd.DataFrame()\n",
    "\n",
    "lang_pd['Language'] = list_lang\n",
    "\n",
    "lang_pd['MO'] = list_mo\n",
    "lang_pd['ED'] = list_ed\n",
    "lang_pd['SP'] = list_sp\n",
    "lang_pd['NA'] = list_na\n",
    "lang_pd['NE'] = list_ne\n",
    "\n",
    "lang_pd_mo = lang_pd[['Language', 'MO']]\n",
    "lang_pd_mo.sort_values(by='MO', inplace=True)\n",
    "\n",
    "lang_pd_ed = lang_pd[['Language', 'ED']]\n",
    "lang_pd_ed.sort_values(by='ED', inplace=True)\n",
    "\n",
    "lang_pd_sp = lang_pd[['Language', 'SP']]\n",
    "lang_pd_sp.sort_values(by='SP', inplace=True)\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.style.use('default')\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 15), sharey=True)\n",
    "fig.suptitle('Interesting class distribution', fontsize='20')\n",
    "fig.supxlabel('Languages', fontsize='20')\n",
    "\n",
    "\n",
    "\n",
    "axs[0].yaxis.grid()\n",
    "axs[0].set_title('MO File Distribution', fontsize='20')\n",
    "axs[0].set_xticklabels(labels = lang_pd_mo[lang_pd_mo['MO']!=0]['Language'], rotation=45, ha='right', fontsize='20')\n",
    "axs[0].set_ylabel('File count', fontsize='20')\n",
    "axs[0].set_yscale('log')\n",
    "\n",
    "\n",
    "axs[1].yaxis.grid()\n",
    "axs[1].set_title('ED File Distribution', fontsize='20')\n",
    "axs[1].set_xticklabels(labels = lang_pd_ed[lang_pd_ed['ED']!=0]['Language'], rotation=45, ha='right', fontsize='20')\n",
    "axs[1].set_yscale('log')\n",
    "\n",
    "axs[2].yaxis.grid()\n",
    "axs[2].set_title('SP File Distribution', fontsize='20')\n",
    "axs[2].set_xticklabels(labels = lang_pd_sp[lang_pd_sp['SP']!=0]['Language'], rotation=45, ha='right', fontsize='20')   \n",
    "axs[2].set_yscale('log')\n",
    "\n",
    "colors_MO=[]\n",
    "for i in lang_pd_mo[lang_pd_mo['MO']!=0]['Language']:\n",
    "    if i in ['cpp','json', 'scala', 'gradle', 'gemfile', 'xml', 'kt', 'yml', 'yaml', 'js', 'ipynb', 'txt']:\n",
    "        colors_MO.append('tab:orange')\n",
    "    else:\n",
    "        colors_MO.append('b')\n",
    "        \n",
    "colors_ED=[]\n",
    "for i in lang_pd_ed[lang_pd_ed['ED']!=0]['Language']:\n",
    "    if i in ['cpp','json', 'scala', 'gradle', 'gemfile', 'xml', 'kt', 'yml', 'yaml', 'js', 'ipynb', 'txt']:\n",
    "        colors_ED.append('tab:orange')\n",
    "    else:\n",
    "        colors_ED.append('b')\n",
    "        \n",
    "colors_SP=[]\n",
    "for i in lang_pd_sp[lang_pd_sp['SP']!=0]['Language']:\n",
    "    if i in ['cpp','json', 'scala', 'gradle', 'gemfile', 'xml', 'kt', 'yml', 'yaml', 'js', 'ipynb', 'txt']:\n",
    "        colors_SP.append('tab:orange')\n",
    "    else:\n",
    "        colors_SP.append('b')\n",
    "        \n",
    "        \n",
    "axs[0].bar(lang_pd_mo[lang_pd_mo['MO']!=0]['Language'], lang_pd_mo[lang_pd_mo['MO']!=0]['MO'], color=colors_MO)\n",
    "\n",
    "axs[1].bar(lang_pd_ed[lang_pd_ed['ED']!=0]['Language'], lang_pd_ed[lang_pd_ed['ED']!=0]['ED'], color=colors_ED)\n",
    "\n",
    "axs[2].bar(lang_pd_sp[lang_pd_sp['SP']!=0]['Language'], lang_pd_sp[lang_pd_sp['SP']!=0]['SP'], color=colors_SP)\n",
    "\n",
    "fig.savefig(\"bugFixFileDistr.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_hist_data_hunk(ext, total_hunk_ext_classes):\n",
    "    hec = total_hunk_ext_classes[ext]\n",
    "    labels = ['MO', 'ED', 'SP', 'NA']\n",
    "    hist = []\n",
    "    for i in labels:\n",
    "        try:\n",
    "            hist.append(hec[i])\n",
    "        except:\n",
    "            hist.append(0)\n",
    "\n",
    "    return hist\n",
    "\n",
    "def plot_hist(file1, file2, file3, total_hunk_ext_classes):\n",
    "    names = ['MO', 'ED', 'SP', 'NI']\n",
    "    \n",
    "    if len(file3)==0 and len(file2)==0:\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(20, 10), sharey=True)\n",
    "        axs.grid()\n",
    "        axs.set_xlabel(file1)\n",
    "        axs.bar(names, get_hist_data_hunk(file1, total_hunk_ext_classes))\n",
    "    else:    \n",
    "        fig, axs = plt.subplots(1, 3, figsize=(20, 10), sharey=True)\n",
    "        axs[0].grid()\n",
    "        axs[0].set_xlabel(file1)\n",
    "        axs[0].bar(names, get_hist_data_hunk(file1, total_hunk_ext_classes))\n",
    "        axs[1].grid()\n",
    "        axs[1].set_xlabel(file2)\n",
    "        axs[1].bar(names, get_hist_data_hunk(file2, total_hunk_ext_classes))\n",
    "        axs[2].grid()\n",
    "        axs[2].set_xlabel(file3)\n",
    "        axs[2].bar(names, get_hist_data_hunk(file3, total_hunk_ext_classes))    \n",
    "               \n",
    "plot_hist('c', 'py', 'java', total_hunk_ext_classes)\n",
    "plot_hist('xml', 'scala', 'json', total_hunk_ext_classes)\n",
    "plot_hist('php', 'cpp', 'h', total_hunk_ext_classes)\n",
    "plot_hist('pl', 'kt', 'ipynb', total_hunk_ext_classes)\n",
    "plot_hist('sh', 'yaml', 'rb', total_hunk_ext_classes)\n",
    "plot_hist('gradle', 'yml', 'gemfile', total_hunk_ext_classes)\n",
    "plot_hist('js', '', '', total_hunk_ext_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Apache Kafka to show gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'Repos_results_bugfix'\n",
    "\n",
    "totals_ = {}\n",
    "repo_nr = 0\n",
    "\n",
    "pair_set = set()\n",
    "\n",
    "i = 0\n",
    "prev_pair_id=4\n",
    "\n",
    "all_pr_class_list = []\n",
    "all_counts_list = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    pair_id = file.split('_')[0]\n",
    "    pair_set.add(pair_id)\n",
    "    with open(f'{directory}/{file}', 'rb') as f:\n",
    "        try:\n",
    "            all_results = pickle.load(f)[0]\n",
    "\n",
    "            pr_class = totals.final_class(all_results)\n",
    "            all_pr_class_list.append(pr_class)\n",
    "            all_counts = totals.count_all_classifications(pr_class)\n",
    "            all_counts_list.append(all_counts)\n",
    "        except EOFError:\n",
    "            continue\n",
    "            \n",
    "dict_final ={}\n",
    "for i in all_pr_class_list:\n",
    "    dict_final=merge_dict(dict_final, i)\n",
    "\n",
    "all_counts_all = dict(functools.reduce(operator.add,map(collections.Counter, all_counts_list)))\n",
    "totals_[pair_id] ={}\n",
    "totals_[pair_id]['pr_class'] = dict_final\n",
    "totals_[pair_id]['all_counts'] = all_counts_all\n",
    "\n",
    "#             with open('Repos_totals/'+ str(repo_file) + '_' + variant1.split('/')[0] + '_' + variant1.split('/')[1] + '_totals.pkl', 'wb') as f:\n",
    "#                 pickle.dump([pr_class, all_counts],f)\n",
    "#             except Exception as e:\n",
    "#                 print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Plotting the distribution of classifications\n",
    "\"\"\"\n",
    "\n",
    "total_all = 0\n",
    "total_mo_all = 0\n",
    "total_ed_all = 0\n",
    "total_sp_all = 0\n",
    "total_cc_all = 0\n",
    "total_ne_all = 0\n",
    "total_error_all = 0\n",
    "total_na_all = 0\n",
    "\n",
    "all_results = []\n",
    "\n",
    "repos = 342\n",
    "for repo_file in totals_:\n",
    "    try:\n",
    "        variant1 = dataset_dict[int(repo_file)]['source']\n",
    "        variant2 = dataset_dict[int(repo_file)]['destination']\n",
    "        variant1_prs = dataset_dict[int(repo_file)]['pr'].split('/')\n",
    "\n",
    "        all_totals= totals_[repo_file]['pr_class']\n",
    "\n",
    "        total_NA = 0\n",
    "        total_ED = 0\n",
    "        total_MO = 0\n",
    "        total_CC = 0\n",
    "        total_SP= 0\n",
    "        total_NE = 0\n",
    "        total_ERROR = 0\n",
    "\n",
    "        for pr in all_totals:\n",
    "            verdict = all_totals[pr]['class']\n",
    "            if verdict == 'ED':\n",
    "                total_ED += 1\n",
    "            elif verdict =='MO':\n",
    "                total_MO += 1\n",
    "            elif verdict == 'SP':\n",
    "                total_SP += 1\n",
    "            elif verdict == 'NA':\n",
    "                total_NA += 1\n",
    "            elif verdict == 'CC':\n",
    "                total_CC += 1\n",
    "            elif verdict =='NE':\n",
    "                total_NE += 1\n",
    "            elif verdict == 'ERROR':\n",
    "                total_ERROR += 1\n",
    "                \n",
    "            total_mid = total_MO+ total_ED + total_SP\n",
    "            total_all += total_mid\n",
    "            \n",
    "        total_total =len(variant1_prs)\n",
    "        \n",
    "        total_mo_all += total_MO\n",
    "        total_ed_all += total_ED\n",
    "        total_sp_all += total_SP\n",
    "        total_na_all += total_NA\n",
    "\n",
    "        total_cc_all += total_CC\n",
    "        total_ne_all += total_NE\n",
    "        total_error_all += total_ERROR\n",
    "\n",
    "        totals_list = [total_MO, total_ED, total_SP, total_CC, total_NE, total_NA, total_ERROR]\n",
    "\n",
    "#         analysis.all_class_bar(totals_list, repo_file, variant1, variant2, False)\n",
    "#         analysis.all_class_pie(totals_list, repo_file, variant1, variant2, False)\n",
    "\n",
    "        all_results.append([variant1, variant2, repo_file, total_MO, total_ED, total_SP, total_CC, total_NE, total_NA, total_ERROR])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(variant1)\n",
    "        print(\"Exception: \", e, '\\n\\n')\n",
    "\n",
    "# header = [\"variant1\", \"variant2\", \"repo_file\", \"total_MO\", \"total_ED\", \"total_SP\", \"total_CC\", \"total_NE\", \"total_NA\",\" total_ERROR\"]\n",
    "# with open(\"results_16_05_22.csv\", \"w\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     # write the header\n",
    "#     writer.writerow(header)\n",
    "\n",
    "#     # write multiple rows\n",
    "#     writer.writerows(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MO = ',total_mo_all)\n",
    "print('ED = ',total_ed_all)\n",
    "print('SP = ',total_sp_all)\n",
    "print('NI = ', total_na_all)\n",
    "\n",
    "print('CC = ',total_cc_all)\n",
    "print('NE = ',total_ne_all)\n",
    "print('ERROR = ', total_error_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "total_hist = [total_mo_all, total_ed_all, total_sp_all, total_na_all, total_cc_all, total_ne_all, total_error_all]\n",
    "\n",
    "left = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "plt.figure(figsize=(15,15), dpi=80)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.bar(left, total_hist, tick_label = ['MO', 'ED', 'SP', 'NI', 'CC', 'NE', 'ERROR'], width = 0.8, color = [\"#e41a1c\",\"#377eb8\",\"#4daf4a\",\"#984ea3\", \"#ff7f00\", \"#ffff33\", \"#a65628\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_classified = total_mo_all+total_ed_all+total_sp_all+total_na_all+total_cc_all+total_error_all+total_ne_all\n",
    "print(f'total classified = {total_classified}')\n",
    "print(f'total interesting = {100*(total_mo_all+total_ed_all+total_sp_all)/total_classified}%')\n",
    "print(f'total unintersting = {100*total_na_all/total_classified}%')\n",
    "print(f'total error = {100*total_error_all/total_classified}%')\n",
    "print(f'total non existant = {100*total_ne_all/total_classified}%')\n",
    "print(f'total cannot classify = {100*total_cc_all/total_classified}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Methods.classifier as classifier\n",
    "\n",
    "classifications = {}\n",
    "\n",
    "directory = 'Repos_results_bugfix'\n",
    "for file in os.listdir(directory):\n",
    "    try:\n",
    "        with open(f'{directory}/{file}', 'rb') as f:\n",
    "            results_file = pickle.load(f)[0]\n",
    "\n",
    "            pair_id = file.split('_')[0]\n",
    "\n",
    "            if pair_id not in classifications:\n",
    "                classifications[pair_id] = {}\n",
    "\n",
    "            for i in results_file:\n",
    "                classifications[pair_id][i] = {}\n",
    "\n",
    "                try:\n",
    "                    for file_exec in results_file[i]:\n",
    "                        ext = file_exec.split('.')[-1]\n",
    "                        classifications[pair_id][i][file_exec]={}\n",
    "                        classifications[pair_id][i][file_exec]['ext'] = ext\n",
    "                        classifications[pair_id][i][file_exec]['hunks'] = []\n",
    "                        classifications[pair_id][i][file_exec]['class'] = ''\n",
    "                        \n",
    "                        patchClass = results_file[i][file_exec]['result']['patchClass']\n",
    "                        \n",
    "                        classifications[pair_id][i][file_exec]['class'] = patchClass\n",
    "                        \n",
    "                        if patchClass in ['MO', 'ED', 'SP', 'NA']:\n",
    "                            hunk_matches_buggy = results_file[i][file_exec]['result']['hunkMatchesBuggy']\n",
    "                            hunk_matches_patch = results_file[i][file_exec]['result']['hunkMatchesPatch']\n",
    "\n",
    "                            hunk_classifications = []\n",
    "\n",
    "                            if len(hunk_matches_buggy) == 0 or len(hunk_matches_patch)==0:\n",
    "                                if len(hunk_matches_buggy) != 0:\n",
    "                                    for patch_nr in hunk_matches_buggy:\n",
    "                                        class_buggy = hunk_matches_buggy[patch_nr]['class']\n",
    "                                        class_patch = 'MC'\n",
    "\n",
    "                                        hunk_class = classifier.classify_hunk(class_buggy, class_patch)\n",
    "                                        hunk_classifications.append(hunk_class)\n",
    "                                elif len(hunk_matches_patch) != 0:\n",
    "                                    for patch_nr in hunk_matches_buggy:\n",
    "                                        class_buggy = 'MC'\n",
    "                                        class_patch = hunk_matches_patch[patch_nr]['class']\n",
    "\n",
    "                                        hunk_class = classifier.classify_hunk(class_buggy, class_patch)\n",
    "                                        hunk_classifications.append(hunk_class)\n",
    "                                else:\n",
    "                                    class_buggy = 'MC'\n",
    "                                    class_patch = 'MC'\n",
    "                                    hunk_class = classifier.classify_hunk(class_buggy, class_patch)\n",
    "                                    hunk_classifications.append(hunk_class)\n",
    "\n",
    "                            elif len(hunk_matches_buggy) == len(hunk_matches_patch):\n",
    "                                for patch_nr in hunk_matches_buggy:\n",
    "                                    class_buggy = hunk_matches_buggy[patch_nr]['class']\n",
    "                                    class_patch = hunk_matches_patch[patch_nr]['class']\n",
    "\n",
    "                                    hunk_class = classifier.classify_hunk(class_buggy, class_patch)\n",
    "                                    hunk_classifications.append(hunk_class)\n",
    "                            classifications[pair_id][i][file_exec]['hunks'].append(hunk_classifications)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print('Pair: ', pair_id)\n",
    "                    print('file: ', file_exec)\n",
    "                    print('PR: ', i)\n",
    "                    print(e)\n",
    "#                     print(results_file[i][file_exec]['result'])\n",
    "    except EOFError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_file_classes = {'ED':0, 'MO': 0,'SP':0, 'NA':0, 'NE':0, 'CC':0, 'ERROR':0}\n",
    "total_hunk_classes = {'ED':0, 'MO': 0,'SP':0, 'NA':0,}\n",
    "\n",
    "empty_class = 0\n",
    "\n",
    "for pair in classifications:\n",
    "    for pr in classifications[pair]:\n",
    "        for file in classifications[pair][pr]:\n",
    "            class_ = classifications[pair][pr][file]['class']\n",
    "            if class_ == 'OTHER EXT':\n",
    "                class_= 'CC'\n",
    "            if class_ == 'NOT EXISTING':\n",
    "                class_ = 'NE'\n",
    "            try:\n",
    "                total_file_classes[class_] += 1\n",
    "            except:\n",
    "                empty_class += 1\n",
    "            for h_list in classifications[pair][pr][file]['hunks']:\n",
    "                for h in h_list:\n",
    "                    total_hunk_classes[h] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_hist = [total_file_classes['MO'], total_file_classes['ED'], total_file_classes['SP'], total_file_classes['NA'], total_file_classes['NE'], total_file_classes['CC'], total_file_classes['ERROR']]\n",
    "\n",
    "left = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "plt.figure(figsize=(15,15), dpi=80)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.bar(left, total_hist, tick_label = ['MO', 'ED', 'SP', 'NI', 'NE', 'CC', 'ERROR'], width = 0.8, color = [\"#e41a1c\",\"#377eb8\",\"#4daf4a\",\"#984ea3\", \"#ff7f00\", \"#ffff33\", \"#a65628\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_hist = [total_hunk_classes['MO'], total_hunk_classes['ED'], total_hunk_classes['SP'], total_hunk_classes['NA']]\n",
    "\n",
    "left = [1, 2, 3, 4]\n",
    "\n",
    "plt.figure(figsize=(15,15), dpi=80)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.bar(left, total_hist, tick_label = ['MO', 'ED', 'SP', 'NI'], width = 0.8, color = [\"#e41a1c\",\"#377eb8\",\"#4daf4a\",\"#984ea3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_file_ext_classes = {} #{'ED':0, 'MO': 0,'SP':0, 'NA':0, 'NE':0, 'CC':0, 'ERROR':0}\n",
    "total_hunk_ext_classes = {} #{'ED':0, 'MO': 0,'SP':0, 'NA':0,}\n",
    "\n",
    "py_files = []\n",
    "\n",
    "for pair in classifications:\n",
    "    for pr in classifications[pair]:\n",
    "        for file in classifications[pair][pr]:\n",
    "            class_ = classifications[pair][pr][file]['class']\n",
    "            ext = classifications[pair][pr][file]['ext']\n",
    "            hunks = classifications[pair][pr][file]['hunks']\n",
    "            \n",
    "            if ext not in total_file_ext_classes:\n",
    "                total_file_ext_classes[ext] = {}\n",
    "            \n",
    "            if class_ == 'OTHER EXT':\n",
    "                class_= 'CC'\n",
    "            if class_ == 'NOT EXISTING':\n",
    "                class_ = 'NE'\n",
    "                \n",
    "            if ext == 'py':\n",
    "                py_files.append(class_)\n",
    "            try:\n",
    "                total_file_ext_classes[ext][class_]+=1\n",
    "            except:\n",
    "                total_file_ext_classes[ext][class_] = 1\n",
    "\n",
    "            if ext not in total_hunk_ext_classes:\n",
    "                total_hunk_ext_classes[ext] = {}\n",
    "            for h_list in hunks:\n",
    "                for h in h_list:\n",
    "                    if h not in total_hunk_ext_classes[ext]:\n",
    "                        total_hunk_ext_classes[ext][h] = 1\n",
    "                    total_hunk_ext_classes[ext][h] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_hist_data(file, total_file_ext_classes):\n",
    "    fec = total_file_ext_classes[file]\n",
    "    labels = ['MO', 'ED', 'SP', 'NA', 'NE', 'CC', 'ERROR']\n",
    "    hist = []\n",
    "    for i in labels:\n",
    "        try:\n",
    "            hist.append(fec[i])\n",
    "        except:\n",
    "            hist.append(0)\n",
    "\n",
    "    return hist\n",
    "\n",
    "def plot_hist(file1, file2, file3, total_file_ext_classes):\n",
    "    names = ['MO', 'ED', 'SP', 'NI', 'NE', 'CC', 'ERROR']\n",
    "    \n",
    "    if len(file3)==0 and len(file2)==0:\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(20, 10), sharey=True)\n",
    "        axs.grid()\n",
    "        axs.set_xlabel(file1)\n",
    "        axs.bar(names, get_hist_data(file1, total_file_ext_classes))\n",
    "    else:    \n",
    "        fig, axs = plt.subplots(1, 3, figsize=(20, 10), sharey=True)\n",
    "        axs[0].grid()\n",
    "        axs[0].set_xlabel(file1)\n",
    "        axs[0].bar(names, get_hist_data(file1, total_file_ext_classes))\n",
    "        axs[1].grid()\n",
    "        axs[1].set_xlabel(file2)\n",
    "        axs[1].bar(names, get_hist_data(file2, total_file_ext_classes))\n",
    "        axs[2].grid()\n",
    "        axs[2].set_xlabel(file3)\n",
    "        axs[2].bar(names, get_hist_data(file3, total_file_ext_classes))    \n",
    "               \n",
    "plot_hist('py', 'java', 'sh', total_hunk_ext_classes)\n",
    "plot_hist('xml', 'scala', 'json', total_hunk_ext_classes)\n",
    "plot_hist('yaml', 'gradle', 'yml', total_hunk_ext_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_hist_data_hunk(ext, total_hunk_ext_classes):\n",
    "    hec = total_hunk_ext_classes[ext]\n",
    "    labels = ['MO', 'ED', 'SP', 'NA']\n",
    "    hist = []\n",
    "    for i in labels:\n",
    "        try:\n",
    "            hist.append(hec[i])\n",
    "        except:\n",
    "            hist.append(0)\n",
    "\n",
    "    return hist\n",
    "\n",
    "def plot_hist(file1, file2, file3, total_hunk_ext_classes):\n",
    "    names = ['MO', 'ED', 'SP', 'NI']\n",
    "    \n",
    "    if len(file3)==0 and len(file2)==0:\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(20, 10), sharey=True)\n",
    "        axs.grid()\n",
    "        axs.set_xlabel(file1)\n",
    "        axs.bar(names, get_hist_data_hunk(file1, total_hunk_ext_classes))\n",
    "    else:    \n",
    "        fig, axs = plt.subplots(1, 3, figsize=(20, 10), sharey=True)\n",
    "        axs[0].grid()\n",
    "        axs[0].set_xlabel(file1)\n",
    "        axs[0].bar(names, get_hist_data_hunk(file1, total_hunk_ext_classes))\n",
    "        axs[1].grid()\n",
    "        axs[1].set_xlabel(file2)\n",
    "        axs[1].bar(names, get_hist_data_hunk(file2, total_hunk_ext_classes))\n",
    "        axs[2].grid()\n",
    "        axs[2].set_xlabel(file3)\n",
    "        axs[2].bar(names, get_hist_data_hunk(file3, total_hunk_ext_classes))    \n",
    "               \n",
    "plot_hist('py', 'java', 'sh', total_hunk_ext_classes)\n",
    "plot_hist('xml', 'scala', 'json', total_hunk_ext_classes)\n",
    "plot_hist('yaml', 'gradle', 'yml', total_hunk_ext_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hunk_ext_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(classifications['4'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys from previous run\n",
    "run1 = ['8102', '8106', '8109', '8111', '8113', '8116', '8119', '8124', '8126', '8130', '8133', '8138', '8140', '8142', '8145', '8154', '8156', '8158', '8165', '8170', '8177', '8178', '8179', '8180', '8181', '8186', '8195', '8197', '8198', '8200', '8203', '8209', '8224', '8226', '8227', '8232', '8233', '8234', '8239', '8241', '8243', '8247', '8256', '8257', '8258', '8259', '8260', '8272', '8273', '8280', '8285', '8287', '8289', '8290', '8291', '8298', '8300', '8301', '8302', '8305', '8310', '8314', '8317', '8319', '8324', '8327', '8330', '8341', '8346', '8348', '8360', '8367', '8382', '8389', '8390', '8392', '8393', '8399', '8400', '8403', '8411', '8412', '8416', '8417', '8419', '8420', '8423', '8424', '8428', '8432', '8435', '8436', '8442', '8447', '8452', '8454', '8459', '8463', '8465', '8476', '8477', '8478', '8479', '8488', '8492', '8493', '8502', '8503', '8504', '8507', '8515', '8524', '8532', '8533', '8536', '8548', '8549', '8552', '8557', '8560', '8561', '8562', '8566', '8567', '8571', '8572', '8579', '8580', '8581', '8584', '8586', '8587', '8599', '8600', '8603', '8609', '8618', '8623', '8625', '8628', '8635', '8646', '8651', '8658', '8659', '8662', '8665', '8672', '8675', '8689', '8693', '8700', '8702', '8705', '8708', '8712', '8716', '8720', '8728', '8730', '8731', '8735', '8743', '8747', '8748', '8749', '8750', '8764', '8772', '8774', '8777', '8779', '8782', '8785', '8786', '8789', '8790', '8797', '8799', '8800', '8802', '8804', '8805', '8809', '8811', '8812', '8815', '8817', '8822', '8823', '8827', '8829', '8841', '8843', '8844', '8848', '8854', '8857', '8858', '8865', '8866', '8870', '8872', '8873', '8874', '8880', '8883', '8884', '8886', '8894', '8900', '8905', '8910', '8913', '8914', '8917', '8918', '8919', '8922', '8927', '8929', '8944', '8951', '8956', '8959', '8963', '8973', '8980', '8981', '8983', '8985', '8987', '8990', '8998', '9003', '9020', '9022', '9023', '9027', '9029', '9033', '9034', '9053', '9056', '9059', '9061', '9071', '9075', '9079', '9086', '9087', '9091', '9095', '9106', '9112', '9121', '9122', '9127', '9140', '9142', '9143', '9153', '9163', '9166', '9172', '9178', '9180', '9183', '9184', '9191', '9198', '9199', '9202', '9203', '9205', '9217', '9218', '9219', '9236', '9245', '9262', '9265', '9267', '9277', '9279', '9280', '9284', '9285', '9294', '9295', '9299', '9301', '9304', '9305', '9308', '9309', '9312', '9313', '9321', '9323', '9330', '9334', '9341', '9344', '9354', '9356', '9357', '9364', '9372', '9374', '9375', '9385', '9390', '9391', '9392', '9393', '9407', '9413', '9417', '9423', '9425', '9426', '9428', '9431', '9432', '9433', '9443', '9446', '9447', '9450', '9458', '9460', '9462', '9466', '9479', '9486', '9487', '9489', '9503', '9508', '9521', '9524', '9530', '9538', '9541', '9544', '9559', '9562', '9563', '9568', '9570', '9586', '9596', '9598', '9599', '9603', '9605', '9614', '9629', '9636', '9649', '9655', '9665', '9671', '9673', '9676', '9678', '9681', '9682', '9683', '9688', '9691', '9718', '9723', '9730', '9733', '9745', '9751', '9752', '9753', '9757', '9759', '9762', '9765', '9768', '9776', '9778', '9781', '9784', '9792', '9800', '9807', '9810', '9812', '9818', '9821', '9827', '9834', '9837', '9843', '9862', '9873', '9875', '9885', '9887', '9893', '9897', '9906', '9911', '9923', '9930', '9935', '9939', '9940', '9942', '9943', '9947', '9948', '9950', '9956', '9965', '9966', '9968', '9974', '9978', '9987', '9990', '10000', '10006', '10014', '10017', '10020', '10024', '10029', '10032', '10033', '10034', '10042', '10052', '10059', '10060', '10068', '10072', '10073', '10074', '10082', '10087', '10088', '10102', '10103', '10104', '10114', '10117', '10127', '10130', '10133', '10134', '10136', '10141', '10143', '10144', '10146', '10153', '10158', '10181', '10193', '10196', '10204', '10210', '10213', '10223', '10226', '10231', '10237', '10243', '10250', '10260', '10264', '10268', '10272', '10292', '10297', '10301', '10304', '10312', '10320', '10321', '10322', '10331', '10334', '10344', '10355', '10361', '10364', '10369', '10372', '10374', '10386', '10387', '10392', '10396', '10398', '10400', '10402', '10404', '10407', '10413', '10423', '10433', '10444', '10445', '10449', '10452', '10453', '10462', '10469', '10474', '10486', '10495', '10503', '10505', '10517', '10529', '10536', '10551', '10553', '10558', '10560', '10561', '10571', '10575', '10584', '10586', '10597', '10604', '10611', '10612', '10621', '10630', '10631', '10632', '10639', '10643', '10657', '10688', '10693', '10694', '10703', '10704', '10706', '10707', '10709', '10713', '10717', '10719', '10735', '10742', '10749', '10759', '10766', '10770', '10776', '10777', '10780', '10794', '10795', '10796', '10797', '10809', '10818', '10822', '10836', '10843', '10858', '10860', '10861', '10877', '10879', '10883', '10885', '10886', '10888', '10890', '10901', '10908', '10915', '10916', '10917', '10923', '10925', '10939', '10941', '10943', '10945', '10958', '10966', '10974', '10975', '10984', '10985', '10989', '10991', '10995', '10998', '11001', '11016', '11019', '11021', '11022', '11026', '11035', '11043', '11048', '11055', '11056', '11065', '11069', '11071', '11090', '11091', '11092', '11093', '11095', '11103', '11108', '11111', '11118', '11122', '11129']\n",
    "\n",
    "run2 = ['8700', '8702', '8705', '8708', '8712', '8716', '8720', '8728', '8730', '8731', '8735', '8743', '8747', '8748', '8749', '8750', '8764', '8772', '8774', '8777', '8779', '8782', '8785', '8786', '8789', '8790', '8797', '8799', '8800', '8802', '8804', '8805', '8809', '8811', '8812', '8815', '8817', '8822', '8823', '8827', '8829', '8841', '8843', '8844', '8848', '8854', '8857', '8858', '8865', '8866', '8870', '8872', '8873', '8874', '8880', '8883', '8884', '8886', '8894', '8900', '8905', '8910', '8913', '8914', '8917', '8918', '8919', '8922', '8927', '8929', '8944', '8951', '8956', '8959', '8963', '8973', '8980', '8981', '8983', '8985', '8987', '8990', '8998', '9003', '9020', '9305', '9308', '9309', '9312', '9313', '9321', '9323', '9330', '9334', '9341', '9344', '9354', '9356', '9357', '9364', '9372', '9374', '9375', '9385', '9390', '9391', '9392', '9393', '9407', '9413', '9417', '9423', '9425', '9426', '9428', '9431', '9432', '9433', '9443', '9446', '9447', '9450', '9458', '9460', '9462', '9466', '9479', '9486', '9487', '9489', '9503', '9508', '9521', '9524', '9530', '9538', '9541', '9544', '9559', '9562', '9563', '9568', '9570', '9586', '9596', '9598', '9599', '9603', '9605', '9614', '9629', '9636', '9649', '9655', '9665', '9671', '9673', '9676', '9678', '9681', '9682', '9683', '9688', '9691', '9718', '9723', '9730', '9733', '9745', '9751', '9752', '9753', '9757', '9759', '9762', '9765', '9768', '9776', '9778', '9781', '9784', '9792', '9800', '9807', '9810', '9812', '9818', '9821', '9827', '9834', '9837']\n",
    "\n",
    "commonPrs = set(run1).intersection(run2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_details = []\n",
    "\n",
    "for i in run2:\n",
    "    comparison_details.append([i,totals_['4']['pr_class'][i]['class']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"PR run2\", \"class run2\"]\n",
    "with open(\"comparisonApacheKafka2.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    # write the header\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # write multiple rows\n",
    "    writer.writerows(comparison_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find distribution of languages in different pairs and select 1 to illustrate the language gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_lang = {}\n",
    "\n",
    "directory = 'Repos_results_bugfix'\n",
    "for file in os.listdir(directory):\n",
    "    try:\n",
    "        with open(f'{directory}/{file}', 'rb') as f:\n",
    "            results_file = pickle.load(f)[0]\n",
    "\n",
    "            pair_id = file.split('_')[0]\n",
    "\n",
    "            if pair_id not in pair_lang:\n",
    "                pair_lang[pair_id] = {}\n",
    "                pair_lang[pair_id]['extensions'] = {}\n",
    "                pair_lang[pair_id]['all counts'] = []\n",
    "                \n",
    "            for i in results_file:\n",
    "                try:\n",
    "                    for file_exec in results_file[i]:\n",
    "                        ext = file_exec.split('.')[-1]\n",
    "                        if ext not in pair_lang[pair_id]:\n",
    "                            pair_lang[pair_id]['extensions'][ext] = 1\n",
    "                        else:\n",
    "                            pair_lang[pair_id]['extensions'][ext] += 1\n",
    "                except Exception as e:\n",
    "                    print('Pair: ', pair_id)\n",
    "                    print('file: ', file_exec)\n",
    "                    print('PR: ', i)\n",
    "                    print(e)\n",
    "        \n",
    "            pr_class = totals.final_class(results_file)\n",
    "            all_counts = totals.count_all_classifications(pr_class)\n",
    "            pair_lang[pair_id]['all counts'] = all_counts  \n",
    "    except EOFError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_lang_distr = {}\n",
    "\n",
    "directory = 'Repos_results_bugfix'\n",
    "for file in os.listdir(directory):\n",
    "    try:\n",
    "        with open(f'{directory}/{file}', 'rb') as f:\n",
    "            results_file = pickle.load(f)[0]\n",
    "\n",
    "            pair_id = file.split('_')[0]\n",
    "\n",
    "            if pair_id not in pair_lang_distr:\n",
    "                pair_lang_distr[pair_id] = {}\n",
    "                pair_lang_distr[pair_id]['extensions'] = {\n",
    "                    'json':0,\n",
    "                    'php': 0,\n",
    "                    'py':0,\n",
    "                    'c':0,\n",
    "                    'rb':0,\n",
    "                    'scala':0,\n",
    "                    'gradle':0,\n",
    "                    'gemfile':0,\n",
    "                    'Gemfile':0,\n",
    "                    'xml':0,\n",
    "                    'h':0,\n",
    "                    'java':0,\n",
    "                    'sh':0,\n",
    "                    'yaml':0,\n",
    "                    'pl':0,\n",
    "                    'kt':0,\n",
    "                    'yml':0,\n",
    "                    'cpp':0,\n",
    "                    'js':0,\n",
    "                }\n",
    "                pair_lang_distr[pair_id]['all counts'] = []\n",
    "            \n",
    "            for i in results_file:\n",
    "                try:\n",
    "                    for file_exec in results_file[i]:\n",
    "                        ext = file_exec.split('.')[-1]\n",
    "                        \n",
    "                    if ext in pair_lang_distr[pair_id]['extensions']:\n",
    "                        pair_lang_distr[pair_id]['extensions'][ext] += 1\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print('Pair: ', pair_id)\n",
    "                    print('file: ', file_exec)\n",
    "                    print('PR: ', i)\n",
    "                    print(e)\n",
    "        \n",
    "            pr_class = totals.final_class(results_file)\n",
    "            all_counts = totals.count_all_classifications(pr_class)\n",
    "            pair_lang_distr[pair_id]['all counts'] = all_counts  \n",
    "    except EOFError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_lang_distr_filter = []\n",
    "for p in pair_lang_distr:\n",
    "    sum_interesting = pair_lang_distr[p]['all counts']['MO'] +  pair_lang_distr[p]['all counts']['ED'] + pair_lang_distr[p]['all counts']['SP']\n",
    "    \n",
    "    if sum_interesting > 6:\n",
    "        pair_lang_distr_filter.append([p,pair_lang_distr[p]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_lang_distr_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Repos_prs_bugfix/11\"+ '_' + dataset_dict[11]['source'].split('/')[0] + '_' + dataset_dict[11]['source'].split('/')[1] + '_prs.pkl', 'rb') as f:\n",
    "    prs = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language gain example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dict(dict1, dict2):\n",
    "    return {**dict1, **dict2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'Language_gain/Repos_results_concrete_example'\n",
    "\n",
    "totals_ = {}\n",
    "repo_nr = 0\n",
    "\n",
    "pair_set = set()\n",
    "\n",
    "i = 0\n",
    "prev_pair_id=11\n",
    "\n",
    "all_pr_class_list = []\n",
    "all_counts_list = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    pair_id = file.split('_')[0]\n",
    "    pair_set.add(pair_id)\n",
    "    with open(f'{directory}/{file}', 'rb') as f:\n",
    "        try:\n",
    "            all_results = pickle.load(f)[0]\n",
    "\n",
    "            pr_class = totals.final_class(all_results)\n",
    "            all_pr_class_list.append(pr_class)\n",
    "            all_counts = totals.count_all_classifications(pr_class)\n",
    "            all_counts_list.append(all_counts)\n",
    "        except EOFError:\n",
    "            continue\n",
    "            \n",
    "dict_final ={}\n",
    "for i in all_pr_class_list:\n",
    "    dict_final=merge_dict(dict_final, i)\n",
    "\n",
    "all_counts_all = dict(functools.reduce(operator.add,map(collections.Counter, all_counts_list)))\n",
    "totals_[pair_id] ={}\n",
    "totals_[pair_id]['pr_class'] = dict_final\n",
    "totals_[pair_id]['all_counts'] = all_counts_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_['11']['all_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results = 'Repos_results_bugfix/11_linkedin_kafka_results.pkl'\n",
    "\n",
    "with open(new_results, 'rb') as f:\n",
    "    all_results_new = pickle.load(f)[0]\n",
    "    pr_class_new = totals.final_class(all_results_new)\n",
    "    all_counts_new = totals.count_all_classifications(pr_class_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_new = {}\n",
    "totals_new['11'] ={}\n",
    "totals_new['11']['pr_class'] = pr_class_new\n",
    "totals_new['11']['all_counts'] = all_counts_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_new['11']['all_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['PR', 'PrevClass', 'NewClass', 'Gain']\n",
    "\n",
    "keys = totals_['11']['pr_class'].keys()\n",
    "csv_data = []\n",
    "for i in keys:\n",
    "    pr = i\n",
    "    \n",
    "    pr_old_class = totals_['11']['pr_class'][i]['class']\n",
    "    pr_new_class = totals_new['11']['pr_class'][i]['class']\n",
    "    \n",
    "    gain = False\n",
    "    if pr_old_class == 'CC' and pr_new_class in ['ED', 'MO', 'SP']:\n",
    "        gain = True\n",
    "    \n",
    "    csv_data.append([pr, pr_old_class, pr_new_class, gain])\n",
    "    \n",
    "# with open('language_gain_apache_kafka.csv', 'w') as f:\n",
    "#     write = csv.writer(f)\n",
    "      \n",
    "#     write.writerow(header)\n",
    "#     write.writerows(csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "language_gain_df=pd.DataFrame(csv_data, columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "language_gain_df[language_gain_df['PrevClass']=='NE'][language_gain_df['NewClass']!='NE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_new['114']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_['11']['pr_class']['243']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_new['11']['pr_class']['243']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_counts = list(totals_['11']['all_counts'].values())\n",
    "new_counts = list(totals_new['11']['all_counts'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(totals_['11']['all_counts'].keys()))\n",
    "print(old_counts)\n",
    "print(list(totals_new['11']['all_counts'].keys()))\n",
    "print(new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "y0 = [44, 5, 3, 18, 52, 39]\n",
    "y1 = [70, 7, 2, 43,  1, 37]\n",
    "\n",
    "x = 2\n",
    "width = 0.3\n",
    "\n",
    "w0 = np.arange(len(y0))\n",
    "w1 = [x + width for x in w0]\n",
    "\n",
    "plt.figure(figsize=(15,10), dpi=80)\n",
    "\n",
    "patch1 = mpatches.Patch(color=\"b\", label='Run-1')\n",
    "patch2 = mpatches.Patch(color=\"turquoise\", label='Run-2')\n",
    "plt.legend(fontsize=18, loc=\"upper left\", handles = [patch1, patch2])\n",
    "\n",
    "plt.bar(w0, y0, color='b', width=width, edgecolor='white', label='Run-1')\n",
    "plt.bar(w1, y1, color='turquoise', width=width, edgecolor='white', label='Run-2')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right', title='source')\n",
    "\n",
    "plt.xlabel('Patch Classifications', fontsize=20)\n",
    "plt.title('Apache Kafka Language Gain (Run-1 vs Run-2)', fontsize=20)\n",
    "plt.ylabel('Patch count', fontsize=20)\n",
    "plt.xticks(fontsize = 14)   \n",
    "plt.yticks(fontsize = 14) \n",
    "plt.xticks([r + width for r in range(len(y0))], \n",
    "           ['MO', 'ED', 'SP', 'NI', 'CC', 'NE'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df2=pd.DataFrame([['MO','Run-1', 44], ['ED', 'Run-1', 5], ['SP','Run-1', 3] , ['NI', 'Run-1', 18], ['CC', 'Run-1',52], ['NE','Run-1', 39], \\\n",
    "                  ['MO','Run-2', 70], ['ED', 'Run-2', 7], ['SP','Run-2', 2] , ['NI', 'Run-2', 43], ['CC', 'Run-2', 1], ['NE','Run-2', 37]] ,columns=['class', 'source', 'apache'])\n",
    "#set seaborn plotting aesthetics\n",
    "\n",
    "# sns.set()\n",
    "sns.set(style='white', font_scale = 1.5)\n",
    "# df2 = pd.read_csv('dataset1.csv',sep=';')\n",
    "#create grouped bar chart\n",
    "g = sns.barplot(x='class', y='apache', palette='bright', hue='source', data=df2,\n",
    "            # palette=[colors[1], colors[5]]\n",
    "               )\n",
    "\n",
    "#add overall title\n",
    "#add axis titles\n",
    "plt.xlabel('Classifications')\n",
    "plt.ylabel('Number of patches')\n",
    "plt.title('Apache Kafka Language Gain (Run-1 vs Run-2)')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right', title='source')\n",
    "g.figure.set_size_inches(9,5)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.grid(which='major', axis='y', linestyle='-')\n",
    "ax.grid(which='major', axis='x', linestyle='')\n",
    "\n",
    "gg = g.get_figure()\n",
    "gg.savefig(\"apache.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Methods.classifier as classifier\n",
    "\n",
    "classifications = {}\n",
    "\n",
    "directory = 'Repos_results_bugfix'\n",
    "for file in os.listdir(directory):\n",
    "    try:\n",
    "        with open(f'{directory}/{file}', 'rb') as f:\n",
    "            results_file = pickle.load(f)[0]\n",
    "\n",
    "            pair_id = file.split('_')[0]\n",
    "\n",
    "            if pair_id not in classifications:\n",
    "                classifications[pair_id] = {}\n",
    "\n",
    "            for i in results_file:\n",
    "                classifications[pair_id][i] = {}\n",
    "\n",
    "                try:\n",
    "                    for file_exec in results_file[i]:\n",
    "                        ext = file_exec.split('.')[-1]\n",
    "                        classifications[pair_id][i][file_exec]={}\n",
    "                        classifications[pair_id][i][file_exec]['ext'] = ext\n",
    "#                         classifications[pair_id][i][file_exec]['hunks'] = []\n",
    "                        classifications[pair_id][i][file_exec]['class'] = ''\n",
    "                        \n",
    "                        patchClass = results_file[i][file_exec]['result']['patchClass']\n",
    "                        \n",
    "                        classifications[pair_id][i][file_exec]['class'] = patchClass\n",
    "                        \n",
    "#                         if patchClass in ['MO', 'ED', 'SP', 'NA']:\n",
    "#                             hunk_matches_buggy = results_file[i][file_exec]['result']['hunkMatchesBuggy']\n",
    "#                             hunk_matches_patch = results_file[i][file_exec]['result']['hunkMatchesPatch']\n",
    "\n",
    "#                             hunk_classifications = []\n",
    "\n",
    "#                             if len(hunk_matches_buggy) == 0 or len(hunk_matches_patch)==0:\n",
    "#                                 if len(hunk_matches_buggy) != 0:\n",
    "#                                     for patch_nr in hunk_matches_buggy:\n",
    "#                                         class_buggy = hunk_matches_buggy[patch_nr]['class']\n",
    "#                                         class_patch = 'MC'\n",
    "\n",
    "#                                         hunk_class = classifier.classify_hunk(class_buggy, class_patch)\n",
    "#                                         hunk_classifications.append(hunk_class)\n",
    "#                                 elif len(hunk_matches_patch) != 0:\n",
    "#                                     for patch_nr in hunk_matches_buggy:\n",
    "#                                         class_buggy = 'MC'\n",
    "#                                         class_patch = hunk_matches_patch[patch_nr]['class']\n",
    "\n",
    "#                                         hunk_class = classifier.classify_hunk(class_buggy, class_patch)\n",
    "#                                         hunk_classifications.append(hunk_class)\n",
    "#                                 else:\n",
    "#                                     class_buggy = 'MC'\n",
    "#                                     class_patch = 'MC'\n",
    "#                                     hunk_class = classifier.classify_hunk(class_buggy, class_patch)\n",
    "#                                     hunk_classifications.append(hunk_class)\n",
    "\n",
    "#                             elif len(hunk_matches_buggy) == len(hunk_matches_patch):\n",
    "#                                 for patch_nr in hunk_matches_buggy:\n",
    "#                                     class_buggy = hunk_matches_buggy[patch_nr]['class']\n",
    "#                                     class_patch = hunk_matches_patch[patch_nr]['class']\n",
    "\n",
    "#                                     hunk_class = classifier.classify_hunk(class_buggy, class_patch)\n",
    "#                                     hunk_classifications.append(hunk_class)\n",
    "#                             classifications[pair_id][i][file_exec]['hunks'].append(hunk_classifications)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print('Pair: ', pair_id)\n",
    "                    print('file: ', file_exec)\n",
    "                    print('PR: ', i)\n",
    "                    print(e)\n",
    "#                     print(results_file[i][file_exec]['result'])\n",
    "    except EOFError:\n",
    "        continue\n",
    "        \n",
    "total_file_ext_classes = {} #{'ED':0, 'MO': 0,'SP':0, 'NA':0, 'NE':0, 'CC':0, 'ERROR':0}\n",
    "# total_hunk_ext_classes = {} #{'ED':0, 'MO': 0,'SP':0, 'NA':0,}\n",
    "\n",
    "py_files = []\n",
    "\n",
    "for pair in classifications:\n",
    "    for pr in classifications[pair]:\n",
    "        for file in classifications[pair][pr]:\n",
    "            class_ = classifications[pair][pr][file]['class']\n",
    "            ext = classifications[pair][pr][file]['ext']\n",
    "#             hunks = classifications[pair][pr][file]['hunks']\n",
    "            \n",
    "            if ext not in total_file_ext_classes:\n",
    "                total_file_ext_classes[ext] = {}\n",
    "            \n",
    "            if class_ == 'OTHER EXT':\n",
    "                class_= 'CC'\n",
    "            if class_ == 'NOT EXISTING':\n",
    "                class_ = 'NE'\n",
    "                \n",
    "            try:\n",
    "                total_file_ext_classes[ext][class_]+=1\n",
    "            except:\n",
    "                total_file_ext_classes[ext][class_] = 1\n",
    "\n",
    "#             if ext not in total_hunk_ext_classes:\n",
    "#                 total_hunk_ext_classes[ext] = {}\n",
    "#             for h_list in hunks:\n",
    "#                 for h in h_list:\n",
    "#                     if h not in total_hunk_ext_classes[ext]:\n",
    "#                         total_hunk_ext_classes[ext][h] = 1\n",
    "#                     total_hunk_ext_classes[ext][h] += 1\n",
    "                    \n",
    "\n",
    "def get_hist_data(file, total_file_ext_classes):\n",
    "    fec = total_file_ext_classes[file]\n",
    "    labels = ['MO', 'ED', 'SP', 'NA', 'NE', 'CC', 'ERROR']\n",
    "    hist = []\n",
    "    for i in labels:\n",
    "        try:\n",
    "            hist.append(fec[i])\n",
    "        except:\n",
    "            hist.append(0)\n",
    "\n",
    "    return hist\n",
    "\n",
    "def plot_hist(file1, file2, file3, total_file_ext_classes):\n",
    "    names = ['MO', 'ED', 'SP', 'NI', 'NE', 'CC', 'ERROR']\n",
    "    \n",
    "    if len(file3)==0 and len(file2)==0:\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(20, 10), sharey=True)\n",
    "        axs.grid()\n",
    "        axs.set_xlabel(file1)\n",
    "        axs.bar(names, get_hist_data(file1, total_file_ext_classes))\n",
    "    else:    \n",
    "        fig, axs = plt.subplots(1, 3, figsize=(20, 10), sharey=True)\n",
    "        axs[0].grid()\n",
    "        axs[0].set_xlabel(file1)\n",
    "        axs[0].bar(names, get_hist_data(file1, total_file_ext_classes))\n",
    "        axs[1].grid()\n",
    "        axs[1].set_xlabel(file2)\n",
    "        axs[1].bar(names, get_hist_data(file2, total_file_ext_classes))\n",
    "        axs[2].grid()\n",
    "        axs[2].set_xlabel(file3)\n",
    "        axs[2].bar(names, get_hist_data(file3, total_file_ext_classes))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist('py', 'java', 'sh', total_hunk_ext_classes)\n",
    "plot_hist('xml', 'scala', 'json', total_hunk_ext_classes)\n",
    "plot_hist('yaml', 'gradle', 'yml', total_hunk_ext_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dict(dict1, dict2):\n",
    "    return {**dict1, **dict2}\n",
    "\n",
    "\n",
    "with open(\"sorted_dataset_opt.pkl\", \"rb\") as f:\n",
    "    dataset_dict = pickle.load(f)\n",
    "    \n",
    "directory = 'Repos_results'\n",
    "\n",
    "totals_ = {}\n",
    "repo_nr = 0\n",
    "\n",
    "pair_set = set()\n",
    "prs_set={}\n",
    "\n",
    "i = 0\n",
    "prev_pair_id = '0'\n",
    "\n",
    "all_pr_class_list = []\n",
    "all_counts_list = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    pair_id = file.split('_')[0]\n",
    "    pair_set.add(pair_id)\n",
    "    \n",
    "    if pair_id not in prs_set:\n",
    "        prs_set[pair_id] = set()\n",
    "    \n",
    "    if pair_id != prev_pair_id:\n",
    "        dict_final ={}\n",
    "        for i in all_pr_class_list:\n",
    "            dict_final=merge_dict(dict_final, i)\n",
    "        try:\n",
    "            all_counts_all = dict(functools.reduce(operator.add,map(collections.Counter, all_counts_list)))\n",
    "            totals_[pair_id] ={}\n",
    "            totals_[pair_id]['pr_class'] = dict_final\n",
    "            totals_[pair_id]['all_counts'] = all_counts_all\n",
    "        except:\n",
    "            totals_[pair_id] ={}\n",
    "            totals_[pair_id]['pr_class'] = dict_final\n",
    "            totals_[pair_id]['all_counts'] = []\n",
    "            \n",
    "        all_pr_class_list = []\n",
    "        all_counts_list = []\n",
    "        \n",
    "        prev_pair_id = pair_id\n",
    "    with open(f'{directory}/{file}', 'rb') as f:\n",
    "        try:\n",
    "            all_results = pickle.load(f)[0]\n",
    "\n",
    "            pr_class = totals.final_class(all_results)\n",
    "            all_pr_class_list.append(pr_class)\n",
    "            all_counts = totals.count_all_classifications(pr_class)\n",
    "            all_counts_list.append(all_counts)\n",
    "            \n",
    "            for i in all_results:\n",
    "                prs_set[pair_id].add(i)\n",
    "                \n",
    "        except EOFError:\n",
    "            continue\n",
    "#             with open('Repos_totals/'+ str(repo_file) + '_' + variant1.split('/')[0] + '_' + variant1.split('/')[1] + '_totals.pkl', 'wb') as f:\n",
    "#                 pickle.dump([pr_class, all_counts],f)\n",
    "#             except Exception as e:\n",
    "#                 print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_ = 0 \n",
    "for i in prs_set:\n",
    "    len_+=len(prs_set[i])\n",
    "len_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#Create mapping between original file and file created by me\n",
    "directory = 'Repos_results/'\n",
    "\n",
    "with open('New_data/dataset_opt.pkl', 'rb') as orig:\n",
    "    data_all = pickle.load(orig)\n",
    "        \n",
    "mapping = {}\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    if os.path.isfile(file_path):        \n",
    "        name_splitted = filename.split('_')\n",
    "        pair_id = int(re.findall(r'\\d+', name_splitted[0])[0])\n",
    "\n",
    "        source = dataset_dict[pair_id]['source']\n",
    "        dest = dataset_dict[pair_id]['destination']\n",
    "            \n",
    "        mapping[pair_id] = {}\n",
    "        for repo_pair in data_all:\n",
    "            if data_all[repo_pair]['mainline'] == source:\n",
    "                if data_all[repo_pair]['variant'] == dest:\n",
    "                    mapping[pair_id]['repo_id'] = repo_pair\n",
    "                    mapping[pair_id]['mainline-variant'] = True\n",
    "            elif data_all[repo_pair]['mainline'] == dest:\n",
    "                if data_all[repo_pair]['variant'] == source:\n",
    "                    mapping[pair_id]['repo_id'] = repo_pair\n",
    "                    \n",
    "                    mapping[pair_id]['mainline-variant'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Plotting the distribution of classifications\n",
    "\"\"\"\n",
    "\n",
    "total_all = 0\n",
    "total_mo_all = 0\n",
    "total_ed_all = 0\n",
    "total_sp_all = 0\n",
    "total_cc_all = 0\n",
    "total_ne_all = 0\n",
    "total_error_all = 0\n",
    "total_na_all = 0\n",
    "\n",
    "all_results = []\n",
    "\n",
    "repos = 342\n",
    "for repo_file in totals_:\n",
    "    try:\n",
    "        variant1 = dataset_dict[int(repo_file)]['source']\n",
    "        variant2 = dataset_dict[int(repo_file)]['destination']\n",
    "        variant1_prs = dataset_dict[int(repo_file)]['pr'].split('/')\n",
    "        \n",
    "        mainlineToVariant = mapping[int(repo_file)]['mainline-variant']\n",
    "        all_totals= totals_[repo_file]['pr_class']\n",
    "\n",
    "        total_NA = 0\n",
    "        total_ED = 0\n",
    "        total_MO = 0\n",
    "        total_CC = 0\n",
    "        total_SP= 0\n",
    "        total_NE = 0\n",
    "        total_ERROR = 0\n",
    "\n",
    "        for pr in all_totals:\n",
    "            verdict = all_totals[pr]['class']\n",
    "            if verdict == 'ED':\n",
    "                total_ED += 1\n",
    "            elif verdict =='MO':\n",
    "                total_MO += 1\n",
    "            elif verdict == 'SP':\n",
    "                total_SP += 1\n",
    "            elif verdict == 'NA':\n",
    "                total_NA += 1\n",
    "            elif verdict == 'CC':\n",
    "                total_CC += 1\n",
    "            elif verdict =='NE':\n",
    "                total_NE += 1\n",
    "            elif verdict == 'ERROR':\n",
    "                total_ERROR += 1\n",
    "                \n",
    "            total_mid = total_MO+ total_ED + total_SP\n",
    "            total_all += total_mid\n",
    "            \n",
    "        total_total =len(variant1_prs)\n",
    "        \n",
    "        total_mo_all += total_MO\n",
    "        total_ed_all += total_ED\n",
    "        total_sp_all += total_SP\n",
    "        total_na_all += total_NA\n",
    "\n",
    "        total_cc_all += total_CC\n",
    "        total_ne_all += total_NE\n",
    "        total_error_all += total_ERROR\n",
    "\n",
    "        totals_list = [total_MO, total_ED, total_SP, total_CC, total_NE, total_NA, total_ERROR]\n",
    "\n",
    "#         analysis.all_class_bar(totals_list, repo_file, variant1, variant2, False)\n",
    "#         analysis.all_class_pie(totals_list, repo_file, variant1, variant2, False)\n",
    "\n",
    "        all_results.append([variant1, variant2, mainlineToVariant, repo_file, total_MO, total_ED, total_SP, total_CC, total_NE, total_NA, total_ERROR])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(variant1)\n",
    "        print(\"Exception: \", e, '\\n')\n",
    "\n",
    "# header = [\"variant1\", \"variant2\",\"mainlineToVariant\", \"repo_file\", \"total_MO\", \"total_ED\", \"total_SP\", \"total_CC\", \"total_NE\", \"total_NA\",\" total_ERROR\"]\n",
    "# with open(\"results_opt_29_05_22.csv\", \"w\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     # write the header\n",
    "#     writer.writerow(header)\n",
    "\n",
    "#     # write multiple rows\n",
    "#     writer.writerows(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MO = ',total_mo_all)\n",
    "print('ED = ',total_ed_all)\n",
    "print('SP = ',total_sp_all)\n",
    "print('NI = ', total_na_all)\n",
    "\n",
    "print('CC = ',total_cc_all)\n",
    "print('NE = ',total_ne_all)\n",
    "print('ERROR = ', total_error_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "total_hist = [total_mo_all, total_ed_all, total_sp_all, total_na_all, total_cc_all, total_ne_all, total_error_all]\n",
    "\n",
    "left = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "plt.figure(figsize=(15,15), dpi=80)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.bar(left, total_hist, tick_label = ['MO', 'ED', 'SP', 'NI', 'CC', 'NE', 'ERROR'], width = 0.8, color = [\"#e41a1c\",\"#377eb8\",\"#4daf4a\",\"#984ea3\", \"#ff7f00\", \"#ffff33\", \"#a65628\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_classified = total_mo_all+total_ed_all+total_sp_all+total_na_all+total_cc_all+total_ne_all\n",
    "print(f'total classified = {total_classified}')\n",
    "print(f'total interesting = {100*(total_mo_all+total_ed_all+total_sp_all)/total_classified}%')\n",
    "print(f'total unintersting = {100*total_na_all/total_classified}%')\n",
    "print(f'total error = {100*total_error_all/total_classified}%')\n",
    "print(f'total non existant = {100*total_ne_all/total_classified}%')\n",
    "print(f'total cannot classify = {100*total_cc_all/total_classified}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File classification distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import Methods.classifier as classifier\n",
    "\n",
    "classifications = {}\n",
    "\n",
    "directory = 'Repos_results'\n",
    "for file in os.listdir(directory):\n",
    "    try:\n",
    "        with open(f'{directory}/{file}', 'rb') as f:\n",
    "            results_file = pickle.load(f)[0]\n",
    "\n",
    "            pair_id = file.split('_')[0]\n",
    "\n",
    "            if pair_id not in classifications:\n",
    "                classifications[pair_id] = {}\n",
    "\n",
    "            for i in results_file:\n",
    "                classifications[pair_id][i] = {}\n",
    "\n",
    "                try:\n",
    "                    for file_exec in results_file[i]:\n",
    "                        ext = file_exec.split('.')[-1]\n",
    "                        classifications[pair_id][i][file_exec]={}\n",
    "                        classifications[pair_id][i][file_exec]['ext'] = ext\n",
    "                        classifications[pair_id][i][file_exec]['hunks'] = []\n",
    "                        classifications[pair_id][i][file_exec]['class'] = ''\n",
    "                        \n",
    "                        patchClass = results_file[i][file_exec]['result']['patchClass']\n",
    "                        \n",
    "                        classifications[pair_id][i][file_exec]['class'] = patchClass\n",
    "                        \n",
    "                        if patchClass in ['MO', 'ED', 'SP', 'NA']:\n",
    "                            hunk_matches_buggy = results_file[i][file_exec]['result']['hunkMatchesBuggy']\n",
    "                            hunk_matches_patch = results_file[i][file_exec]['result']['hunkMatchesPatch']\n",
    "\n",
    "                            hunk_classifications = []\n",
    "\n",
    "                            if len(hunk_matches_buggy) == 0 or len(hunk_matches_patch)==0:\n",
    "                                if len(hunk_matches_buggy) != 0:\n",
    "                                    for patch_nr in hunk_matches_buggy:\n",
    "                                        class_buggy = hunk_matches_buggy[patch_nr]['class']\n",
    "                                        class_patch = 'MC'\n",
    "\n",
    "                                        hunk_class = classifier.classify_hunk(class_buggy, class_patch)\n",
    "                                        hunk_classifications.append(hunk_class)\n",
    "                                elif len(hunk_matches_patch) != 0:\n",
    "                                    for patch_nr in hunk_matches_buggy:\n",
    "                                        class_buggy = 'MC'\n",
    "                                        class_patch = hunk_matches_patch[patch_nr]['class']\n",
    "\n",
    "                                        hunk_class = classifier.classify_hunk(class_buggy, class_patch)\n",
    "                                        hunk_classifications.append(hunk_class)\n",
    "                                else:\n",
    "                                    class_buggy = 'MC'\n",
    "                                    class_patch = 'MC'\n",
    "                                    hunk_class = classifier.classify_hunk(class_buggy, class_patch)\n",
    "                                    hunk_classifications.append(hunk_class)\n",
    "\n",
    "                            elif len(hunk_matches_buggy) == len(hunk_matches_patch):\n",
    "                                for patch_nr in hunk_matches_buggy:\n",
    "                                    class_buggy = hunk_matches_buggy[patch_nr]['class']\n",
    "                                    class_patch = hunk_matches_patch[patch_nr]['class']\n",
    "\n",
    "                                    hunk_class = classifier.classify_hunk(class_buggy, class_patch)\n",
    "                                    hunk_classifications.append(hunk_class)\n",
    "                            classifications[pair_id][i][file_exec]['hunks'].append(hunk_classifications)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print('Pair: ', pair_id)\n",
    "                    print('file: ', file_exec)\n",
    "                    print('PR: ', i)\n",
    "                    print(e)\n",
    "#                     print(results_file[i][file_exec]['result'])\n",
    "    except EOFError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_file_classes = {'ED':0, 'MO': 0,'SP':0, 'NA':0, 'NE':0, 'CC':0, 'ERROR':0}\n",
    "total_hunk_classes = {'ED':0, 'MO': 0,'SP':0, 'NA':0,}\n",
    "lang_distr = {}\n",
    "\n",
    "for pair in classifications:\n",
    "    for pr in classifications[pair]:\n",
    "        for file in classifications[pair][pr]:\n",
    "            class_ = classifications[pair][pr][file]['class']\n",
    "            if  classifications[pair][pr][file]['ext'] not in lang_distr:\n",
    "                lang_distr[classifications[pair][pr][file]['ext']] = 1\n",
    "            else:\n",
    "                lang_distr[classifications[pair][pr][file]['ext']] += 1\n",
    "                \n",
    "            if class_ == 'OTHER EXT':\n",
    "                class_= 'CC'\n",
    "            if class_ == 'NOT EXISTING':\n",
    "                class_ = 'NE'\n",
    "            try:\n",
    "                total_file_classes[class_] += 1\n",
    "            except:\n",
    "                print(class_)\n",
    "            \n",
    "            for h_list in classifications[pair][pr][file]['hunks']:\n",
    "                for h in h_list:\n",
    "                    total_hunk_classes[h] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_hist = [total_file_classes['MO'], total_file_classes['ED'], total_file_classes['SP'], total_file_classes['NA'], total_file_classes['NE'], total_file_classes['CC'], total_file_classes['ERROR']]\n",
    "\n",
    "left = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "plt.figure(figsize=(15,15), dpi=80)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.bar(left, total_hist, tick_label = ['MO', 'ED', 'SP', 'NI', 'NE', 'CC', 'ERROR'], width = 0.8, color = [\"#e41a1c\",\"#377eb8\",\"#4daf4a\",\"#984ea3\", \"#ff7f00\", \"#ffff33\", \"#a65628\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_hist = [total_hunk_classes['MO'], total_hunk_classes['ED'], total_hunk_classes['SP'], total_hunk_classes['NA']]\n",
    "\n",
    "left = [1, 2, 3, 4]\n",
    "\n",
    "plt.figure(figsize=(15,15), dpi=80)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.bar(left, total_hist, tick_label = ['MO', 'ED', 'SP', 'NI'], width = 0.8, color = [\"#e41a1c\",\"#377eb8\",\"#4daf4a\",\"#984ea3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_file_ext_classes = {} #{'ED':0, 'MO': 0,'SP':0, 'NA':0, 'NE':0, 'CC':0, 'ERROR':0}\n",
    "total_hunk_ext_classes = {} #{'ED':0, 'MO': 0,'SP':0, 'NA':0,}\n",
    "\n",
    "py_files = []\n",
    "\n",
    "for pair in classifications:\n",
    "    for pr in classifications[pair]:\n",
    "        for file in classifications[pair][pr]:\n",
    "            class_ = classifications[pair][pr][file]['class']\n",
    "            ext = classifications[pair][pr][file]['ext']\n",
    "            hunks = classifications[pair][pr][file]['hunks']\n",
    "            \n",
    "            if ext not in total_file_ext_classes:\n",
    "                total_file_ext_classes[ext] = {}\n",
    "            \n",
    "            if class_ == 'OTHER EXT':\n",
    "                class_= 'CC'\n",
    "            if class_ == 'NOT EXISTING':\n",
    "                class_ = 'NE'\n",
    "                \n",
    "            if ext == 'py':\n",
    "                py_files.append(class_)\n",
    "            try:\n",
    "                total_file_ext_classes[ext][class_]+=1\n",
    "            except:\n",
    "                total_file_ext_classes[ext][class_] = 1\n",
    "\n",
    "            if ext not in total_hunk_ext_classes:\n",
    "                total_hunk_ext_classes[ext] = {}\n",
    "            for h_list in hunks:\n",
    "                for h in h_list:\n",
    "                    if h not in total_hunk_ext_classes[ext]:\n",
    "                        total_hunk_ext_classes[ext][h] = 1\n",
    "                    total_hunk_ext_classes[ext][h] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_file_ext_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_hist_data(file, total_file_ext_classes):\n",
    "    fec = total_file_ext_classes[file]\n",
    "    labels = ['MO', 'ED', 'SP', 'NA', 'NE', 'CC', 'ERROR']\n",
    "    hist = []\n",
    "    for i in labels:\n",
    "        try:\n",
    "            hist.append(fec[i])\n",
    "        except:\n",
    "            hist.append(0)\n",
    "\n",
    "    return hist\n",
    "\n",
    "def plot_hist(file1, file2, file3, total_file_ext_classes):\n",
    "    names = ['MO', 'ED', 'SP', 'NI', 'NE', 'CC', 'ERROR']\n",
    "    \n",
    "    if len(file3)==0 and len(file2)==0:\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(20, 10), sharey=True)\n",
    "        axs.grid()\n",
    "        axs.set_xlabel(file1)\n",
    "        axs.bar(names, get_hist_data(file1, total_file_ext_classes))\n",
    "    elif len(file3)==0:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(20, 10), sharey=True)\n",
    "        axs[0].grid()\n",
    "        axs[0].set_xlabel(file1)\n",
    "        axs[0].bar(names, get_hist_data(file1, total_file_ext_classes))\n",
    "        axs[1].grid()\n",
    "        axs[1].set_xlabel(file2)\n",
    "        axs[1].bar(names, get_hist_data(file2, total_file_ext_classes))\n",
    "    else:    \n",
    "        fig, axs = plt.subplots(1, 3, figsize=(20, 10), sharey=True)\n",
    "        axs[0].grid()\n",
    "        axs[0].set_xlabel(file1)\n",
    "        axs[0].bar(names, get_hist_data(file1, total_file_ext_classes))\n",
    "        axs[1].grid()\n",
    "        axs[1].set_xlabel(file2)\n",
    "        axs[1].bar(names, get_hist_data(file2, total_file_ext_classes))\n",
    "        axs[2].grid()\n",
    "        axs[2].set_xlabel(file3)\n",
    "        axs[2].bar(names, get_hist_data(file3, total_file_ext_classes))    \n",
    "               \n",
    "plot_hist('js', 'py', 'c', total_file_ext_classes)\n",
    "plot_hist('rb', 'scala', 'cpp', total_file_ext_classes)\n",
    "plot_hist('gradle', 'yml', 'ipynb', total_file_ext_classes)\n",
    "plot_hist('xml', 'h', 'java', total_file_ext_classes)\n",
    "plot_hist('sh', 'yaml', '', total_file_ext_classes)\n",
    "# plot_hist('kt', '', '', total_file_ext_classes)\n",
    "# plot_hist('js', '', '', total_file_ext_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hist_data_hunk(ext, total_hunk_ext_classes):\n",
    "    hec = total_hunk_ext_classes[ext]\n",
    "    labels = ['MO', 'ED', 'SP', 'NA']\n",
    "    hist = []\n",
    "    for i in labels:\n",
    "        try:\n",
    "            hist.append(hec[i])\n",
    "        except:\n",
    "            hist.append(0)\n",
    "\n",
    "    return hist\n",
    "\n",
    "def plot_hist(file1, file2, file3, total_hunk_ext_classes):\n",
    "    names = ['MO', 'ED', 'SP', 'NI']\n",
    "    \n",
    "    if len(file3)==0 and len(file2)==0:\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(20, 10), sharey=True)\n",
    "        axs.grid()\n",
    "        axs.set_xlabel(file1)\n",
    "        axs.bar(names, get_hist_data_hunk(file1, total_hunk_ext_classes))\n",
    "    elif len(file3)==0:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(20, 10), sharey=True)\n",
    "        axs[0].grid()\n",
    "        axs[0].set_xlabel(file1)\n",
    "        axs[0].bar(names, get_hist_data_hunk(file1, total_hunk_ext_classes))\n",
    "        axs[1].grid()\n",
    "        axs[1].set_xlabel(file2)\n",
    "        axs[1].bar(names, get_hist_data_hunk(file2, total_hunk_ext_classes))\n",
    "    else:    \n",
    "        fig, axs = plt.subplots(1, 3, figsize=(20, 10), sharey=True)\n",
    "        axs[0].grid()\n",
    "        axs[0].set_xlabel(file1)\n",
    "        axs[0].bar(names, get_hist_data_hunk(file1, total_hunk_ext_classes))\n",
    "        axs[1].grid()\n",
    "        axs[1].set_xlabel(file2)\n",
    "        axs[1].bar(names, get_hist_data_hunk(file2, total_hunk_ext_classes))\n",
    "        axs[2].grid()\n",
    "        axs[2].set_xlabel(file3)\n",
    "        axs[2].bar(names, get_hist_data_hunk(file3, total_hunk_ext_classes))    \n",
    "               \n",
    "plot_hist('js', 'py', 'c', total_hunk_ext_classes)\n",
    "plot_hist('rb', 'scala', 'cpp', total_hunk_ext_classes)\n",
    "plot_hist('gradle', 'yml', 'ipynb', total_hunk_ext_classes)\n",
    "plot_hist('xml', 'h', 'java', total_hunk_ext_classes)\n",
    "plot_hist('sh', 'yaml', '', total_hunk_ext_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# MO ED SP NA NE\n",
    "list_mo=[]\n",
    "list_ed=[]\n",
    "list_sp=[]\n",
    "list_na=[]\n",
    "list_ne=[]\n",
    "list_lang=[]\n",
    "\n",
    "for lang in total_file_ext_classes:\n",
    "    if lang in ['c', 'h', 'java', 'cpp', 'py', 'pl', 'json', 'rb', 'scala', 'sh', \\\n",
    "                'php', 'gradle', 'gemfile', 'xml', 'kt', 'yml', 'yaml', 'js', 'ipynb', 'txt']:\n",
    "        \n",
    "        list_lang.append(lang)\n",
    "        if 'MO' in total_file_ext_classes[lang]:\n",
    "            list_mo.append(total_file_ext_classes[lang]['MO'])\n",
    "        else:\n",
    "            list_mo.append(0)\n",
    "            \n",
    "        if 'ED' in total_file_ext_classes[lang]:    \n",
    "            list_ed.append(total_file_ext_classes[lang]['ED'])\n",
    "        else:\n",
    "            list_ed.append(0)\n",
    "            \n",
    "        if 'SP' in total_file_ext_classes[lang]:\n",
    "            list_sp.append(total_file_ext_classes[lang]['SP'])\n",
    "        else:\n",
    "            list_sp.append(0)\n",
    "            \n",
    "        if 'NA' in total_file_ext_classes[lang]:\n",
    "            list_na.append(total_file_ext_classes[lang]['NA'])\n",
    "        else:\n",
    "            list_na.append(0)\n",
    "            \n",
    "        if 'NE' in total_file_ext_classes[lang]:\n",
    "            list_ne.append(total_file_ext_classes[lang]['NE'])\n",
    "        else:\n",
    "            list_ne.append(0)\n",
    "            \n",
    "lang_pd = pd.DataFrame()\n",
    "\n",
    "lang_pd['Language'] = list_lang\n",
    "\n",
    "lang_pd['MO'] = list_mo\n",
    "lang_pd['ED'] = list_ed\n",
    "lang_pd['SP'] = list_sp\n",
    "lang_pd['NA'] = list_na\n",
    "lang_pd['NE'] = list_ne\n",
    "\n",
    "lang_pd_mo = lang_pd[['Language', 'MO']]\n",
    "lang_pd_mo.sort_values(by='MO', inplace=True)\n",
    "\n",
    "lang_pd_ed = lang_pd[['Language', 'ED']]\n",
    "lang_pd_ed.sort_values(by='ED', inplace=True)\n",
    "\n",
    "lang_pd_sp = lang_pd[['Language', 'SP']]\n",
    "lang_pd_sp.sort_values(by='SP', inplace=True)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 10), sharey=True)\n",
    "fig.suptitle('Interesting class distribution', fontsize='30')\n",
    "fig.supxlabel('Languages', fontsize='20')\n",
    "# fig.supylabel('File Count', fontsize='20')\n",
    "\n",
    "axs[0].yaxis.grid()\n",
    "axs[0].set_title('MO File Distribution', fontsize='30')\n",
    "axs[0].set_xticklabels(labels = lang_pd_mo[lang_pd_mo['MO']!=0]['Language'], rotation=45, ha='right', fontsize='20')\n",
    "# axs[0].set_xlabel('Languages',fontsize='20')\n",
    "axs[0].set_ylabel('File count',fontsize='30')\n",
    "axs[0].set_yscale('log')\n",
    "\n",
    "axs[1].yaxis.grid()\n",
    "axs[1].set_title('ED File Distribution', fontsize='30')\n",
    "axs[1].set_xticklabels(labels = lang_pd_ed[lang_pd_ed['ED']!=0]['Language'], rotation=45, ha='right', fontsize='20')\n",
    "# axs[1].set_xlabel('Languages',fontsize='20')\n",
    "# axs[1].set_ylabel('File count',fontsize='20')\n",
    "axs[1].set_yscale('log')\n",
    "\n",
    "axs[2].yaxis.grid()\n",
    "axs[2].set_title('SP File Distribution', fontsize='30')\n",
    "axs[2].set_xticklabels(labels = lang_pd_sp[lang_pd_sp['SP']!=0]['Language'], rotation=45, ha='right', fontsize='20')   \n",
    "# axs[2].set_xlabel('Languages',fontsize='20')\n",
    "# axs[2].set_ylabel('File count',fontsize='20')\n",
    "axs[2].set_yscale('log')\n",
    "\n",
    "colors_MO=[]\n",
    "for i in lang_pd_mo[lang_pd_mo['MO']!=0]['Language']:\n",
    "    if i in ['cpp','json', 'scala', 'gradle', 'gemfile', 'xml', 'kt', 'yml', 'yaml', 'js', 'ipynb', 'txt']:\n",
    "        colors_MO.append('turquoise')\n",
    "    else:\n",
    "        colors_MO.append('b')\n",
    "        \n",
    "colors_ED=[]\n",
    "for i in lang_pd_ed[lang_pd_ed['ED']!=0]['Language']:\n",
    "    if i in ['cpp','json', 'scala', 'gradle', 'gemfile', 'xml', 'kt', 'yml', 'yaml', 'js', 'ipynb', 'txt']:\n",
    "        colors_ED.append('turquoise')\n",
    "    else:\n",
    "        colors_ED.append('b')\n",
    "        \n",
    "colors_SP=[]\n",
    "for i in lang_pd_sp[lang_pd_sp['SP']!=0]['Language']:\n",
    "    if i in ['cpp','json', 'scala', 'gradle', 'gemfile', 'xml', 'kt', 'yml', 'yaml', 'js', 'ipynb', 'txt']:\n",
    "        colors_SP.append('turquoise')\n",
    "    else:\n",
    "        colors_SP.append('b')\n",
    "        \n",
    "axs[0].bar(lang_pd_mo[lang_pd_mo['MO']!=0]['Language'], lang_pd_mo[lang_pd_mo['MO']!=0]['MO'], color=colors_MO)\n",
    "\n",
    "axs[1].bar(lang_pd_ed[lang_pd_ed['ED']!=0]['Language'], lang_pd_ed[lang_pd_ed['ED']!=0]['ED'], color=colors_ED)\n",
    "\n",
    "axs[2].bar(lang_pd_sp[lang_pd_sp['SP']!=0]['Language'], lang_pd_sp[lang_pd_sp['SP']!=0]['SP'], color=colors_SP)\n",
    "\n",
    "\n",
    "plt.savefig(\"apacheKakfaGain.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lang_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sorted_dataset_opt.pkl\", \"rb\") as f:\n",
    "    dataset_dict = pickle.load(f)\n",
    "    \n",
    "    total_prs_opt = 0\n",
    "    for i in dataset_dict:\n",
    "        total_prs_opt += dataset_dict[i]['pr_no']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get list of prs executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_executed={}\n",
    "\n",
    "directory = 'Repos_results'\n",
    "for file in os.listdir(directory):\n",
    "    try:\n",
    "        with open(f'{directory}/{file}', 'rb') as f:\n",
    "            results_file = pickle.load(f)[0]\n",
    "\n",
    "            pair_id = file.split('_')[0]\n",
    "\n",
    "            if pair_id not in pr_executed:\n",
    "                pr_executed[pair_id] = []\n",
    "            \n",
    "            for i in results_file:\n",
    "                pr_executed[pair_id].append(i)\n",
    "    except EOFError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pr_executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_to_execute={}\n",
    "\n",
    "directory = 'Repos_prs_opt'\n",
    "for file in os.listdir(directory):\n",
    "    try:\n",
    "        with open(f'{directory}/{file}', 'rb') as f:\n",
    "            results_file = pickle.load(f)[0]\n",
    "\n",
    "            pair_id = file.split('_')[0]\n",
    "\n",
    "            if pair_id not in pr_to_execute:\n",
    "                pr_to_execute[pair_id] = []\n",
    "            \n",
    "            for i in results_file:\n",
    "                if i not in pr_executed[pair_id]:\n",
    "                    pr_to_execute[pair_id].append(i)\n",
    "    except EOFError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_to_execute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting for the optimization patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('results_opt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = []\n",
    "count_list = []\n",
    "variant1_list = []\n",
    "variant2_list = []\n",
    "mainlineToVariant_list = []\n",
    "\n",
    "def appendData(row, class_, class_total):\n",
    "    class_list.append(class_)\n",
    "    count_list.append(row[class_total])\n",
    "    variant1_list.append(row['variant1'])\n",
    "    variant2_list.append(row['variant2'])\n",
    "    if row['mainlineToVariant']:\n",
    "        mainlineToVariant_list.append('upstream')\n",
    "    else:\n",
    "        mainlineToVariant_list.append('fork')\n",
    "    \n",
    "for idx, row in data.iterrows():\n",
    "    appendData(row, 'MO', 'total_MO')\n",
    "    appendData(row, 'ED', 'total_ED')\n",
    "    appendData(row, 'SP', 'total_SP')\n",
    "    appendData(row, 'NI', 'total_NA')\n",
    "    appendData(row, 'CC', 'total_CC')\n",
    "    appendData(row, 'NE', 'total_NE')\n",
    "    \n",
    "data_boxplot = pd.DataFrame()\n",
    "\n",
    "data_boxplot['class'] = class_list\n",
    "data_boxplot['count'] = count_list\n",
    "data_boxplot['variant1'] = variant1_list\n",
    "data_boxplot['variant2'] = variant2_list\n",
    "data_boxplot['source'] =mainlineToVariant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_boxplot.to_csv('boxPlot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.5)\n",
    "\n",
    "g = sns.boxplot(x=\"class\", y=\"count\", hue=\"source\", data=data_boxplot, palette=\"bright\")\n",
    "\n",
    "plt.xlabel('Classifications')\n",
    "plt.ylabel('Number of patches')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper left', title='source')\n",
    "g.figure.set_size_inches(9,6)\n",
    "g.set_yscale(\"log\")\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.grid(which='major', axis='y', linestyle='-')\n",
    "ax.grid(which='major', axis='x', linestyle='')\n",
    "\n",
    "gg = g.get_figure()\n",
    "gg.savefig(\"boxplotOpt.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Violin plot for the optmization patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'Repos_results'\n",
    "\n",
    "mainline_list = []\n",
    "variant_list = []\n",
    "pr_list = []\n",
    "filename_list = []\n",
    "operation_list = []\n",
    "file_class_list = []\n",
    "interesting_list = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    with open(f'{directory}/{file}', 'rb') as f:\n",
    "        results_file = pickle.load(f)[0]\n",
    "        \n",
    "        \n",
    "        pair_id = file.split('_')[0]\n",
    "        \n",
    "        for i in results_file:\n",
    "            for file in results_file[i]:\n",
    "                mainline_list.append(dataset_dict[int(pair_id)]['source'])\n",
    "                variant_list.append(dataset_dict[int(pair_id)]['destination'])\n",
    "                pr_list.append(i)\n",
    "                \n",
    "                file_class = results_file[i][file]['result']['patchClass']\n",
    "                \n",
    "                filename_list.append(file)\n",
    "                file_class_list.append(file_class)\n",
    "                if file_class in ['MO', 'ED', 'NA', 'SP']:\n",
    "                    operation_list.append(results_file[i][file]['result']['type'])\n",
    "                    interesting_list.append(1)\n",
    "                else:\n",
    "                    operation_list.append('None')\n",
    "                    interesting_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_violinplot = pd.DataFrame()\n",
    "\n",
    "data_violinplot['Mainline'] = mainline_list\n",
    "data_violinplot['Fork'] = variant_list\n",
    "data_violinplot['Pr nr'] = pr_list\n",
    "data_violinplot['Filename'] = filename_list\n",
    "data_violinplot['Operation'] =operation_list\n",
    "data_violinplot['File classification'] = file_class_list\n",
    "data_violinplot['Interesting'] = interesting_list\n",
    "data_violinplot.to_csv('Details Fig.12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proportion of interesting files.\n",
    "directory = 'Repos_results'\n",
    "\n",
    "repo_list = []\n",
    "pr_list = []\n",
    "patch_list = []\n",
    "interesting_list = []\n",
    "total_list = []\n",
    "interesting_ratio = []\n",
    "\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    with open(f'{directory}/{file}', 'rb') as f:\n",
    "        results_file = pickle.load(f)[0]\n",
    "\n",
    "        pr_class = totals.final_class(results_file)\n",
    "        pair_id = file.split('_')[0]\n",
    "        \n",
    "        for i in results_file:\n",
    "            patch_class = pr_class[i]['class']\n",
    "\n",
    "            interesting_file = pr_class[i]['totals']['total_MO'] + pr_class[i]['totals']['total_ED'] + pr_class[i]['totals']['total_SP']\n",
    "            total_files = pr_class[i]['totals']['total_E']\n",
    "            \n",
    "            if patch_class in ['MO', 'ED', 'SP']:\n",
    "                repo_list.append(pair_id)\n",
    "                pr_list.append(i)\n",
    "                patch_list.append(patch_class)\n",
    "                interesting_list.append(interesting_file)\n",
    "                total_list.append(total_files)\n",
    "                interesting_ratio.append(interesting_file/total_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_violinplot = pd.DataFrame()\n",
    "\n",
    "data_violinplot['repo'] = repo_list\n",
    "data_violinplot['pr'] = pr_list\n",
    "data_violinplot['patchClass'] = patch_list\n",
    "data_violinplot['interstingFilesCount'] = interesting_list\n",
    "data_violinplot['totalFiles'] =total_list\n",
    "data_violinplot['interestingRatio'] = interesting_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_violinplot.to_csv('violionPlot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\", font_scale=1.5)\n",
    "\n",
    "g = sns.violinplot(x=\"patchClass\", y=\"interestingRatio\",\n",
    "                    data=data_violinplot, palette=\"bright\",\n",
    "                    scale=\"count\", inner = 'quartile')\n",
    "sns.set(font_scale = 1.5)\n",
    "\n",
    "plt.xlabel('Classifications')\n",
    "plt.ylabel('Proportion of interesting\\n files in a patch')\n",
    "# plt.grid(True,  linestyle = '--')\n",
    "# plt.legend(loc='upper right', title='source')\n",
    "g.figure.set_size_inches(11.5,6)\n",
    "# g.set_yscale(\"log\")\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.grid(which='major', axis='y', linestyle='--')\n",
    "ax.grid(which='major', axis='x', linestyle='')\n",
    "\n",
    "gg = g.get_figure()\n",
    "gg.savefig(\"violinPlotOpt.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the interesting optimization patches and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = list()\n",
    "token_file = '/mnt/c/Users/User1/Desktop/tokens.txt'\n",
    "\n",
    "with open(token_file) as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[0].split(','):\n",
    "        token_list.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'Repos_results'\n",
    "import Methods.commitLoader as commitLoader\n",
    "actual_pair_list = []\n",
    "mainline_list = []\n",
    "variant_list = []\n",
    "mainline_to_variant_list = []\n",
    "pr_list = []\n",
    "pr_title_list = []\n",
    "patch_class_list = []\n",
    "ct =0\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    with open(f'{directory}/{file}', 'rb') as f:\n",
    "        results_file = pickle.load(f)[0]\n",
    "        \n",
    "        pair_id = file.split('_')[0]\n",
    "        actual_pair_id = mapping[int(pair_id)]['repo_id']\n",
    "        mainline_to_variant = mapping[int(pair_id)]['mainline-variant']\n",
    "        \n",
    "        pr_class = totals.final_class(results_file)\n",
    "        \n",
    "        for i in results_file:\n",
    "            actual_pair_list.append(actual_pair_id)\n",
    "            mainline_list.append(dataset_dict[int(pair_id)]['source'])\n",
    "            variant_list.append(dataset_dict[int(pair_id)]['destination'])\n",
    "            mainline_to_variant_list.append(mainline_to_variant)\n",
    "            pr_list.append(i)\n",
    "            try:\n",
    "                url = f'https://api.github.com/repos/' + dataset_dict[int(pair_id)]['source'] +f'/pulls/{i}'\n",
    "                if ct == len(token_list):\n",
    "                    ct = 0\n",
    "                response = commitLoader.apiRequest(url, token_list[ct])\n",
    "                ct += 1\n",
    "\n",
    "                pr_title_list.append(response['title'])\n",
    "            except:\n",
    "                pr_title_list.append('')\n",
    "            patch_class_list.append(pr_class[i]['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_interesting= pd.DataFrame()\n",
    "\n",
    "data_interesting['Actual Pair'] = actual_pair_list\n",
    "data_interesting['Variant 1 (Source)'] = mainline_list\n",
    "data_interesting['Variant 2 (Target)'] = variant_list\n",
    "data_interesting['Mainline To Variant'] = mainline_to_variant_list\n",
    "data_interesting['PR'] = pr_list\n",
    "data_interesting['PR Title'] = pr_title_list\n",
    "data_interesting['Patch classification (tool)'] = patch_class_list\n",
    "\n",
    "data_interesting.to_csv('Interesting Optimization Patches.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the mapping for the optimization patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Plotting the distribution of classifications\n",
    "\"\"\"\n",
    "data = []\n",
    "\n",
    "total_all = 0\n",
    "total_mo_all = 0\n",
    "total_ed_all = 0\n",
    "total_sp_all = 0\n",
    "total_cc_all = 0\n",
    "total_ne_all = 0\n",
    "total_error_all = 0\n",
    "total_na_all = 0\n",
    "\n",
    "all_results = []\n",
    "\n",
    "repos = 342\n",
    "for repo_file in totals_:\n",
    "    try:\n",
    "        variant1 = dataset_dict[int(repo_file)]['source']\n",
    "        variant2 = dataset_dict[int(repo_file)]['destination']\n",
    "        variant1_prs = dataset_dict[int(repo_file)]['pr'].split('/')\n",
    "        \n",
    "        mainlineToVariant = mapping[int(repo_file)]['mainline-variant']\n",
    "        all_totals= totals_[repo_file]['pr_class']\n",
    "\n",
    "        actua\n",
    "        total_NA = 0\n",
    "        total_ED = 0\n",
    "        total_MO = 0\n",
    "        total_CC = 0\n",
    "        total_SP= 0\n",
    "        total_NE = 0\n",
    "        total_ERROR = 0\n",
    "\n",
    "        for pr in all_totals:\n",
    "            verdict = all_totals[pr]['class']\n",
    "            if verdict == 'ED':\n",
    "                total_ED += 1\n",
    "            elif verdict =='MO':\n",
    "                total_MO += 1\n",
    "            elif verdict == 'SP':\n",
    "                total_SP += 1\n",
    "            elif verdict == 'NA':\n",
    "                total_NA += 1\n",
    "            elif verdict == 'CC':\n",
    "                total_CC += 1\n",
    "            elif verdict =='NE':\n",
    "                total_NE += 1\n",
    "            elif verdict == 'ERROR':\n",
    "                total_ERROR += 1\n",
    "                \n",
    "            total_mid = total_MO+ total_ED + total_SP\n",
    "            total_all += total_mid\n",
    "            \n",
    "        total_total =len(variant1_prs)\n",
    "        \n",
    "        total_mo_all += total_MO\n",
    "        total_ed_all += total_ED\n",
    "        total_sp_all += total_SP\n",
    "        total_na_all += total_NA\n",
    "\n",
    "        total_cc_all += total_CC\n",
    "        total_ne_all += total_NE\n",
    "        total_error_all += total_ERROR\n",
    "\n",
    "        totals_list = [total_MO, total_ED, total_SP, total_CC, total_NE, total_NA, total_ERROR]\n",
    "\n",
    "#         analysis.all_class_bar(totals_list, repo_file, variant1, variant2, False)\n",
    "#         analysis.all_class_pie(totals_list, repo_file, variant1, variant2, False)\n",
    "\n",
    "        data.append([variant1, variant2, mainlineToVariant, repo_file, total_MO, total_ED, total_SP, total_CC, total_NE, total_NA, total_ERROR])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(variant1)\n",
    "        print(\"Exception: \", e, '\\n')\n",
    "\n",
    "# header = [\"variant1\", \"variant2\",\"mainlineToVariant\", \"repo_file\", \"total_MO\", \"total_ED\", \"total_SP\", \"total_CC\", \"total_NE\", \"total_NA\",\" total_ERROR\"]\n",
    "# with open(\"results_opt_29_05_22.csv\", \"w\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     # write the header\n",
    "#     writer.writerow(header)\n",
    "\n",
    "#     # write multiple rows\n",
    "#     writer.writerows(all_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
