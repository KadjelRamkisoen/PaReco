{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./..')\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import concurrent.futures\n",
    "import pickle\n",
    "\n",
    "import Methods.common as common\n",
    "import Methods.dataLoader as dataloader\n",
    "import Methods.commitLoader as commitloader\n",
    "\n",
    "try:\n",
    "    import argparse\n",
    "    import magic\n",
    "except ImportError as err:\n",
    "    print (err)\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = 'sorted_dataset_opt.pkl'\n",
    "token_file = '/mnt/c/Users/User1/Desktop/tokens.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_file, \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "token_list = list()\n",
    "with open(token_file) as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[0].split(','):\n",
    "        token_list.append(line)\n",
    "        \n",
    "lenTokens = len(token_list)\n",
    "\n",
    "try:\n",
    "    common.magic_cookie = magic.open(magic.MAGIC_MIME)\n",
    "    common.magic_cookie.load()\n",
    "except AttributeError:\n",
    "    common.magic_cookie = magic.Magic(mime=True, uncompress=True)\n",
    "common.verbose_print('[-] initialized magic cookie\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getDataUsingThreads(params):\n",
    "    source, destination, prs, cut_off_date, task, begin, end = params\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    ct = 0\n",
    "    \n",
    "    working_repos = []\n",
    "    for pair in range(begin, end):\n",
    "        print(pair, '-', source[pair], '-', destination[pair])\n",
    "        common.verbose_mode = False\n",
    "\n",
    "        try:\n",
    "            destination_sha, ct = dataloader.getDestinationSha(destination[pair], cut_off_date[pair], token_list, ct)\n",
    "            ct, pr_data, req, runtime = dataloader.fetchPrData(source[pair], destination[pair], prs[pair], destination_sha, token_list, ct)\n",
    "            with open('Repos_prs_opt/' + str(pair) + '_' + source[pair].split('/')[0] + '_' + source[pair].split('/')[1] + '_prs.pkl', 'wb') as f:\n",
    "                pickle.dump([pr_data, req, runtime], f)\n",
    "            working_repos.append(pair)\n",
    "        except Exception as e:\n",
    "            print('Exception  :(\\n ',e)\n",
    "    print(working_repos)        \n",
    "    finish = time.perf_counter()\n",
    "    return f'Task {task} done executing in ... {round((finish - start) / 60, 2)} minutes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def mainThread():\n",
    "    source = []\n",
    "    destination = []\n",
    "    prs = []\n",
    "    cut_off_date = []\n",
    "    \n",
    "    for i in dataset:\n",
    "        source.append(dataset[i]['source'])\n",
    "        destination.append(dataset[i]['destination'])\n",
    "        prs.append(str(dataset[i]['pr']).split('/'))\n",
    "        cut_off_date.append(dataset[i]['cut_off_date'])\n",
    "    \n",
    "    task = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    \n",
    "#     start1 = 0\n",
    "#     end1 = 1\n",
    "#     start2 = 1\n",
    "#     end2 = 3\n",
    "#     start6 = 3\n",
    "#     end6 = 7\n",
    "#     start7 = 7\n",
    "#     end7 = 10\n",
    "#     start3 = 10\n",
    "#     end3 = 15\n",
    "#     start4 = 15\n",
    "#     end4 = 31\n",
    "#     start5 = 45\n",
    "#     end5 = 66\n",
    "#     start9 = 48\n",
    "#     end9 = 50\n",
    "#     start10 = 150\n",
    "#     end10 = 342\n",
    "    start10 = 0\n",
    "    end10 = 1\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        params = [\n",
    "#                   (source, destination, prs, cut_off_date, task[0], start1, end1),\n",
    "#                   (source, destination, prs, cut_off_date, task[1], start2, end2),\n",
    "#                   (source, destination, prs, cut_off_date, task[2], start3, end3),\n",
    "#                   (source, destination, prs, cut_off_date, task[3], start4, end4),\n",
    "#                   (source, destination, prs, cut_off_date, task[4], start5, end5)\n",
    "#                   (source, destination, prs, cut_off_date, task[5], start6, end6),\n",
    "#                   (source, destination, prs, cut_off_date, task[6], start7, end7)\n",
    "#                   (source, destination, prs, cut_off_date, task[7], start8, end8)\n",
    "#                   (source, destination, prs, cut_off_date, task[8], start9, end9),\n",
    "                  (source, destination, prs, cut_off_date, task[9], start10, end10)\n",
    "                 ]\n",
    "        results = executor.map(getDataUsingThreads, params)\n",
    "\n",
    "        for result in results:\n",
    "            print(result)\n",
    "\n",
    "mainThread()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To get only the prs used in manual check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,10,10,10,11,11,11,11,11,11,11,11,11,11,12,12,12,12,12,12,12,13,13,13,13,13,13,13,13,14,15,15,15,15,15,16,16,16,16,16,16,18,18,19,20,23,23,23,24,24,24,25,25,26,26,26,26,26,26,27,27,27,28,28,28,28,30,31,31,32,32,34,34,35,35,36,36,38,39,41,41,41,42,42,43,44,46,46,48,48,49,51,51,51,52,53,54,55,56,56,56,58,58,60,60,62,63,63,64,65,65,66,66,67,72,73,74,79,82,86,88,89,90,91,93,94,95,96,97,165,180,183,184]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOnlyManualCheckData(params):\n",
    "    pair_ids, source, destination, prs, cut_off_date = params\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    ct = 0\n",
    "    \n",
    "    working_repos = []\n",
    "    for pair in range(0, len(pair_ids)):\n",
    "        print(pair_ids[pair], '-', source[pair], '-', destination[pair])\n",
    "        common.verbose_mode = False\n",
    "\n",
    "        try:\n",
    "            destination_sha, ct = dataloader.getDestinationSha(destination[pair], cut_off_date[pair], token_list, ct)\n",
    "            ct, pr_data, req, runtime = dataloader.fetchPrData(source[pair], destination[pair], prs[pair], destination_sha, token_list, ct)\n",
    "            with open('Repos_prs/' + str(pair_ids[pair]) + '_' + source[pair].split('/')[0] + '_' + source[pair].split('/')[1] + '_prs.pkl', 'wb') as f:\n",
    "                pickle.dump([pr_data, req, runtime], f)\n",
    "            working_repos.append(pair_ids[pair])\n",
    "        except Exception as e:\n",
    "            print('Exception  :(\\n ',e)\n",
    "    print(working_repos)        \n",
    "    finish = time.perf_counter()\n",
    "    return f'Task done executing in ... {round((finish - start) / 60, 2)} minutes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def onlyManualCheckData():\n",
    "    data = {}\n",
    "    start_idx = 0\n",
    "\n",
    "    for pair in pairs:\n",
    "        if pair not in data:\n",
    "            data[pair] = []\n",
    "        data[pair].append(mc_prs[start_idx])        \n",
    "        start_idx += 1\n",
    "    \n",
    "    \n",
    "    source = []\n",
    "    destination = []\n",
    "    prs = []\n",
    "    cut_off_date = []\n",
    "    pair_ids =[]\n",
    "    \n",
    "    for i in data:\n",
    "        pair_ids.append(i)\n",
    "        source.append(dataset[i]['source'])\n",
    "        destination.append(dataset[i]['destination'])\n",
    "        \n",
    "        prs_i = []\n",
    "        for pr in data[i]:\n",
    "            prs_i.append(str(pr))\n",
    "        prs.append(prs_i)\n",
    "        \n",
    "        cut_off_date.append(dataset[i]['cut_off_date'])\n",
    "\n",
    "    getOnlyManualCheckData((pair_ids, source, destination, prs, cut_off_date))\n",
    "    \n",
    "#     task = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    \n",
    "#     start1 = 0\n",
    "#     end1 = 31\n",
    "#     start2 = 31\n",
    "#     end2 = 61\n",
    "#     start3 = 61\n",
    "#     end3 = 91\n",
    "#     start4 = 91\n",
    "#     end4 = 121\n",
    "#     start5 = 121\n",
    "#     end5 = 151\n",
    "#     start6 = 151\n",
    "#     end6 = 181\n",
    "#     start7 = 181\n",
    "#     end7 = 211\n",
    "#     start8 = 211\n",
    "#     end8 = 241\n",
    "#     start9 = 241\n",
    "#     end9 = 276\n",
    "    \n",
    "#     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#         params = [\n",
    "#                   (source, destination, prs, cut_off_date, task[0], start1, end1),\n",
    "#                   (source, destination, prs, cut_off_date, task[1], start2, end2),\n",
    "#                   (source, destination, prs, cut_off_date, task[2], start3, end3),\n",
    "#                   (source, destination, prs, cut_off_date, task[3], start4, end4),\n",
    "#                   (source, destination, prs, cut_off_date, task[4], start5, end5),\n",
    "#                   (source, destination, prs, cut_off_date, task[5], start6, end6),\n",
    "#                   (source, destination, prs, cut_off_date, task[6], start7, end7),\n",
    "#                   (source, destination, prs, cut_off_date, task[7], start8, end8),\n",
    "#                   (source, destination, prs, cut_off_date, task[8], start9, end9)\n",
    "#                  ]\n",
    "#         results = executor.map(getDataUsingThreads, params)\n",
    "#         for result in results:\n",
    "#             print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyManualCheckData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common.readResults(2, dataset[2]['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_exe = {'0': ['2185', '2479', '2484'],\n",
    " '10': ['8930', '10217'],\n",
    " '11': [],\n",
    " '12': [],\n",
    " '13': ['14350', '14391'],\n",
    " '14': ['232', '509', '655', '698'],\n",
    " '15': ['680', '684', '685', '690', '691', '692'],\n",
    " '16': ['913', '1000', '1047', '1060', '1071', '1195', '1215', '1231'],\n",
    " '17': ['760', '812'],\n",
    " '18': ['307', '318'],\n",
    " '19': ['760', '812', '855', '864', '884'],\n",
    " '1': ['14981', '15000', '15163', '15213', '15227', '15262', '15362'],\n",
    " '20': ['86', '203', '206', '237', '271', '284'],\n",
    " '21': ['30', '32', '33'],\n",
    " '22': ['1928', '2387', '2736', '2737'],\n",
    " '23': ['2739', '2742', '3497'],\n",
    " '24': ['63', '64'],\n",
    " '25': ['409', '773'],\n",
    " '26': ['574', '610', '809'],\n",
    " '27': ['62', '137', '161'],\n",
    " '28': ['258'],\n",
    " '29': ['235', '269'],\n",
    " '2': ['8697'],\n",
    " '30': ['184', '231'],\n",
    " '31': ['2276'],\n",
    " '32': ['173', '188'],\n",
    " '33': ['5758', '7069'],\n",
    " '34': ['1837', '1874'],\n",
    " '35': ['99', '138'],\n",
    " '36': [],\n",
    " '37': ['105', '153'],\n",
    " '38': ['117', '157'],\n",
    " '39': ['88', '105'],\n",
    " '3': ['1068', '1554', '1698'],\n",
    " '40': ['773', '779'],\n",
    " '41': ['1398', '1448'],\n",
    " '42': ['154', '203'],\n",
    " '43': ['199'],\n",
    " '44': ['159'],\n",
    " '45': ['7069'],\n",
    " '46': ['528'],\n",
    " '47': ['55'],\n",
    " '48': ['6077'],\n",
    " '49': ['352'],\n",
    " '4': ['7181', '7528', '7621', '7724', '8311', '8428'],\n",
    " '50': ['1024'],\n",
    " '51': ['1024'],\n",
    " '52': ['509'],\n",
    " '53': ['49'],\n",
    " '54': ['1414'],\n",
    " '55': ['128'],\n",
    " '56': ['611'],\n",
    " '57': [],\n",
    " '58': ['182'],\n",
    " '59': [],\n",
    " '5': [],\n",
    " '60': [],\n",
    " '61': ['528'],\n",
    " '62': ['5221'],\n",
    " '63': ['275'],\n",
    " '64': ['327'],\n",
    " '65': ['327'],\n",
    " '6': ['2344', '2345'],\n",
    " '7': ['4680', '4876', '4911', '4922', '4978', '5141'],\n",
    " '8': ['2320', '2333'],\n",
    " '9': ['7', '9', '14', '18']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct =0\n",
    "pair = '13'\n",
    "source = dataset[int(pair)]['source']\n",
    "destination = dataset[int(pair)]['destination']\n",
    "cut_off_date = dataset[int(pair)]['cut_off_date']\n",
    "prs = str(dataset[int(pair)]['pr']).split('/')\n",
    "# prs = ['49']\n",
    "\n",
    "destination_sha = ''\n",
    "lenTokens = len(token_list)\n",
    "\n",
    "if ct == lenTokens:\n",
    "    ct = 0\n",
    "cut_off_commits = commitloader.apiRequest('https://api.github.com/repos/' + destination +'/commits?until=' + cut_off_date, token_list[ct])\n",
    "ct += 1\n",
    "\n",
    "destination_sha = cut_off_commits[0]['sha']\n",
    "\n",
    "# def fetchP|rData(source, destination, prs, destination_sha, token_list, ct):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(prs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_sha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 53-, 40-, 42-, 31, 36, 38, 39, 27, 28, 21-25\n",
    "\n",
    "start = time.time()\n",
    "req = 0\n",
    "pr_data = {}\n",
    "lenTokens = len(token_list)\n",
    "\n",
    "for k in prs:\n",
    "    if k not in to_exe[pair]:\n",
    "        try:\n",
    "            pr_data[k] = {}\n",
    "\n",
    "    #         print(f'ct1 = {ct}')\n",
    "            # Get the PR\n",
    "            if ct == lenTokens:\n",
    "                ct = 0\n",
    "\n",
    "\n",
    "            pr_request = 'https://api.github.com/repos/' + source + '/pulls/' + k\n",
    "\n",
    "            pr = commitloader.apiRequest(pr_request, token_list[ct])\n",
    "            ct += 1\n",
    "            req += 1\n",
    "\n",
    "    #         print(f'ct2 = {ct}')\n",
    "            # Get the commit\n",
    "            if ct == lenTokens:\n",
    "                ct = 0\n",
    "\n",
    "#             print(pr)\n",
    "            commits_url = pr['commits_url']\n",
    "            commits = commitloader.apiRequest(commits_url, token_list[ct])\n",
    "            common.verbose_print(f'ct ={ct}')\n",
    "            ct += 1\n",
    "            req += 1\n",
    "    #         print(f'ct3 = {ct}')\n",
    "    #         print('Commits\\n', commits)\n",
    "            commits_data = {}\n",
    "\n",
    "            nr_files = pr['changed_files']\n",
    "\n",
    "            pr_data[k]['pr_url'] = pr_request\n",
    "            pr_data[k]['commits_url'] = commits_url\n",
    "            pr_data[k]['changed_files'] = nr_files\n",
    "            pr_data[k]['commits_data'] = list()\n",
    "            pr_data[k]['destination_sha'] = destination_sha\n",
    "\n",
    "            shas = []\n",
    "            parents = []\n",
    "            commits_dates = []\n",
    "\n",
    "            for i in commits:\n",
    "                if ct == lenTokens:\n",
    "                    ct = 0\n",
    "                commit_url = i['url']\n",
    "                commit = commitloader.apiRequest(commit_url, token_list[ct])\n",
    "\n",
    "                ct += 1\n",
    "                req += 1\n",
    "    #             print(f'ct4 = {ct}')\n",
    "\n",
    "#                 print(commit)\n",
    "                sha = commit['sha']\n",
    "                if sha not in shas:\n",
    "                    shas.append(sha)\n",
    "                    parents.append(commit['parents'][0]['sha'])\n",
    "                    commits_dates.append(datetime.strptime(commit['commit']['author']['date'], \"%Y-%m-%dT%H:%M:%SZ\"))\n",
    "\n",
    "                try:\n",
    "                    files = commit['files']\n",
    "                    for j in files:\n",
    "                        status = j['status']\n",
    "                        file_name = j['filename']\n",
    "                        added_lines = j['additions']\n",
    "                        removed_lines = j['deletions']\n",
    "                        changes = j['changes']\n",
    "                        file_ext = commitloader.get_file_type(file_name)\n",
    "\n",
    "                        if file_name not in commits_data:\n",
    "                            commits_data[file_name] = list()\n",
    "                            if ct == lenTokens:\n",
    "                                ct = 0\n",
    "\n",
    "    #                         if file_name=='contrib/win32/win32compat/win32_usertoken_utils.c':\n",
    "    #                             print(file_name)\n",
    "\n",
    "                            if commitloader.findFile(file_name, destination, token_list[ct], destination_sha):\n",
    "                                sub = {}\n",
    "                                sub['commit_url'] = commit_url\n",
    "                                sub['commit_sha'] = commit['sha']\n",
    "                                sub['commit_date'] = commit['commit']['author']['date']\n",
    "                                sub['parent_sha'] = commit['parents'][0]['sha']\n",
    "                                sub['status'] =status\n",
    "                                sub['additions'] = added_lines\n",
    "                                sub['deletions'] = removed_lines\n",
    "                                sub['changes'] = changes\n",
    "\n",
    "                                commits_data[file_name].append(sub)\n",
    "\n",
    "\n",
    "                            ct += 1\n",
    "    #                         print(f'ct5 = {ct}')\n",
    "                        else:\n",
    "                            if ct == lenTokens:\n",
    "                                ct = 0\n",
    "                            if commitloader.findFile(file_name, destination, token_list[ct], destination_sha):\n",
    "                                sub = {}\n",
    "                                sub['commit_url'] = commit_url\n",
    "                                sub['commit_sha'] = commit['sha']\n",
    "                                sub['commit_date'] = commit['commit']['author']['date']\n",
    "                                sub['parent_sha'] = commit['parents'][0]['sha']\n",
    "                                sub['status'] =status\n",
    "                                sub['additions'] = added_lines\n",
    "                                sub['deletions'] = removed_lines\n",
    "                                sub['changes'] = changes\n",
    "\n",
    "                                commits_data[file_name].append(sub)\n",
    "\n",
    "                            ct += 1\n",
    "    #                         print(f'ct6 = {ct}')\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print('This should only happen if there are no files changed in a commit')\n",
    "            pr_data[k]['commits_data'].append(commits_data)\n",
    "\n",
    "            min_date = min(commits_dates)\n",
    "            max_date = max(commits_dates)\n",
    "            min_index = commits_dates.index(min_date)\n",
    "            max_index = commits_dates.index(max_date)\n",
    "            pr_data[k]['first_commit_parent'] = parents[min_index]\n",
    "            pr_data[k]['last_commit_sha'] = shas[max_index]\n",
    "\n",
    "        except Exception as e:\n",
    "            print('An error occurred while fetching the pull request information from GitHub.')\n",
    "            print(pr_request)\n",
    "            print (f'Error has to do with: {e}')\n",
    "\n",
    "end = time.time()\n",
    "runtime = end - start\n",
    "print('Fetch Runtime: ', runtime)\n",
    "\n",
    "with open('Repos_prs_opt/' + str(pair) + '_' + source.split('/')[0] + '_' + source.split('/')[1] + '_prs_to_exe.pkl', 'wb') as f:\n",
    "    pickle.dump([pr_data, req, runtime], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Repos_prs_opt/' + str(pair) + '_' + source.split('/')[0] + '_' + source.split('/')[1] + '_prs_to_exe.pkl', 'wb') as f:\n",
    "    pickle.dump([pr_data, req, runtime], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
